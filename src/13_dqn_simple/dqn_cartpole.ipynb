{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efbbac7",
   "metadata": {
    "papermill": {
     "duration": 0.032188,
     "end_time": "2021-12-04T16:49:24.620327",
     "exception": false,
     "start_time": "2021-12-04T16:49:24.588139",
     "status": "completed"
    },
    "tags": [],
    "id": "4efbbac7"
   },
   "source": [
    "# Aufgabe 13: How to train a Deep Q Network\n",
    "01.02.2022, Thomas Iten\n",
    "\n",
    "**Content**\n",
    "1. Setup\n",
    "2. DQN\n",
    "3. Memory (Experience, ReplayBuffer, RLDataset)\n",
    "4. Agent\n",
    "5. DQN Lightning Module\n",
    "6. Trainer\n",
    "7. Test Score\n",
    "8. Visualize the game\n",
    "\n",
    "**References**\n",
    "- https://github.com/gsurma/cartpole\n",
    "- https://github.com/openai/gym/wiki/Leaderboard\n",
    "- https://colab.research.google.com/drive/1cBfKCEsLcj1ppXfqRvvgoranbxj0C6a9#scrollTo=b0dc7a72\n",
    "- https://colab.research.google.com/drive/1tlbupNEh_MJwYrJZM8FZpJcsKX7GObRj\n",
    "- https://colab.research.google.com/drive/1PY5a-NqvjFxnK_Bq4empyT4GQSNKk9lE?usp=sharing\n",
    "- https://colab.research.google.com/drive/1lsDRQYs87b7BHkSw6BD4m6WMNZS_48i6?usp=sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc6c370",
   "metadata": {
    "papermill": {
     "duration": 0.02782,
     "end_time": "2021-12-04T16:49:24.679755",
     "exception": false,
     "start_time": "2021-12-04T16:49:24.651935",
     "status": "completed"
    },
    "tags": [],
    "id": "efc6c370"
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5894fd9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T16:49:24.744158Z",
     "iopub.status.busy": "2021-12-04T16:49:24.743680Z",
     "iopub.status.idle": "2021-12-04T16:49:27.687873Z",
     "shell.execute_reply": "2021-12-04T16:49:27.687317Z"
    },
    "id": "5894fd9f",
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": 2.979438,
     "end_time": "2021-12-04T16:49:27.688021",
     "exception": false,
     "start_time": "2021-12-04T16:49:24.708583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install --quiet \"torch>=1.6, <1.9\" \"pytorch-lightning>=1.3\" \"gym\" \"torchmetrics>=0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad4092f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T16:49:27.750875Z",
     "iopub.status.busy": "2021-12-04T16:49:27.750375Z",
     "iopub.status.idle": "2021-12-04T16:49:31.622878Z",
     "shell.execute_reply": "2021-12-04T16:49:31.623262Z"
    },
    "papermill": {
     "duration": 3.906565,
     "end_time": "2021-12-04T16:49:31.623443",
     "exception": false,
     "start_time": "2021-12-04T16:49:27.716878",
     "status": "completed"
    },
    "tags": [],
    "id": "ad4092f4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict, deque, namedtuple\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.utilities import DistributedType\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. DQN\n",
    "\n",
    "Wir haben 2 Instanzen vom DQN:\n",
    "- DQN versucht Q-Value zu verbessern. Dass heisst ist supervised, muss lernen kÃ¶nnen.\n",
    "- Ein Netzwerk wird dauernd trainiert mit Target von zweiten Netzwerk, dass nicht trainiert wird.\n",
    "\n",
    "Idee:\n",
    "- Es wird ein zweites Netzwerk definiert, dass nicht trainiert wird,\n",
    "  sondern mit den Gwichten von ersten Netzwerk aktualisiert wird (sync_rate).\n",
    "- Aufgabe vom 2ten Netzwerk ist Target zu generieren. Target ist Next State\n",
    "- Das erste Netzwerk wird mit dem Next State vom 2ten Netzwerk angepasst.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bd3cb4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T16:49:31.685019Z",
     "iopub.status.busy": "2021-12-04T16:49:31.684540Z",
     "iopub.status.idle": "2021-12-04T16:49:31.686662Z",
     "shell.execute_reply": "2021-12-04T16:49:31.686183Z"
    },
    "papermill": {
     "duration": 0.035013,
     "end_time": "2021-12-04T16:49:31.686760",
     "exception": false,
     "start_time": "2021-12-04T16:49:31.651747",
     "status": "completed"
    },
    "tags": [],
    "id": "4bd3cb4e"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size: int, n_actions: int, hidden_size=24):\n",
    "        \"\"\"\n",
    "            obs_size: observation/state size of the environment\n",
    "            n_actions: number of discrete actions available in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012cc8f7",
   "metadata": {
    "papermill": {
     "duration": 0.032792,
     "end_time": "2021-12-04T16:49:31.746872",
     "exception": false,
     "start_time": "2021-12-04T16:49:31.714080",
     "status": "completed"
    },
    "tags": [],
    "id": "012cc8f7"
   },
   "source": [
    "# 3. Memory\n",
    "\n",
    "## Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77510037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T16:49:31.806464Z",
     "iopub.status.busy": "2021-12-04T16:49:31.805941Z",
     "iopub.status.idle": "2021-12-04T16:49:31.808050Z",
     "shell.execute_reply": "2021-12-04T16:49:31.807649Z"
    },
    "papermill": {
     "duration": 0.033474,
     "end_time": "2021-12-04T16:49:31.808151",
     "exception": false,
     "start_time": "2021-12-04T16:49:31.774677",
     "status": "completed"
    },
    "tags": [],
    "id": "77510037"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Named tuple for storing experience steps gathered in training\n",
    "Experience = namedtuple(\n",
    "    \"Experience\",\n",
    "    field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReplayBuffer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ae0c219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T16:49:31.869846Z",
     "iopub.status.busy": "2021-12-04T16:49:31.869370Z",
     "iopub.status.idle": "2021-12-04T16:49:31.871439Z",
     "shell.execute_reply": "2021-12-04T16:49:31.870977Z"
    },
    "papermill": {
     "duration": 0.035508,
     "end_time": "2021-12-04T16:49:31.871534",
     "exception": false,
     "start_time": "2021-12-04T16:49:31.836026",
     "status": "completed"
    },
    "tags": [],
    "id": "8ae0c219"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay Buffer for storing past experiences allowing the agent to learn from them.\n",
    "\n",
    "    Args:\n",
    "        capacity: size of the buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience: Experience) -> None:\n",
    "        \"\"\"Add experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: tuple (state, action, reward, done, new_state)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        # Samplel indices batch_size indicies\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool),\n",
    "            np.array(next_states),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RLDataset\n",
    "\n",
    "Buffer wird in RLDataset gewrappt. Man iteriert, aber jedesmal wird gesampelt!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1052c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T16:49:31.931299Z",
     "iopub.status.busy": "2021-12-04T16:49:31.930829Z",
     "iopub.status.idle": "2021-12-04T16:49:31.932898Z",
     "shell.execute_reply": "2021-12-04T16:49:31.932435Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.033795,
     "end_time": "2021-12-04T16:49:31.932991",
     "exception": false,
     "start_time": "2021-12-04T16:49:31.899196",
     "status": "completed"
    },
    "tags": [],
    "id": "d1052c84"
   },
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences\n",
    "    during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe4a973",
   "metadata": {
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.027797,
     "end_time": "2021-12-04T16:49:31.988622",
     "exception": false,
     "start_time": "2021-12-04T16:49:31.960825",
     "status": "completed"
    },
    "tags": [],
    "id": "fbe4a973"
   },
   "source": [
    "# 4. Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42dd9534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T16:49:32.054816Z",
     "iopub.status.busy": "2021-12-04T16:49:32.054310Z",
     "iopub.status.idle": "2021-12-04T16:49:32.056372Z",
     "shell.execute_reply": "2021-12-04T16:49:32.055906Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.039725,
     "end_time": "2021-12-04T16:49:32.056473",
     "exception": false,
     "start_time": "2021-12-04T16:49:32.016748",
     "status": "completed"
    },
    "tags": [],
    "id": "42dd9534"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Base Agent class handeling the interaction with the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: training environment\n",
    "            replay_buffer: replay buffer storing experiences\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resents the environment and updates the state.\"\"\"\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\n",
    "        \"\"\"Using the given network, decide what action to carry out using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            action\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state = torch.tensor([self.state])\n",
    "\n",
    "            if device not in [\"cpu\"]:\n",
    "                state = state.cuda(device)\n",
    "\n",
    "            q_values = net(state)                      # q_values (kÃ¶nnen auch > 1 sein)\n",
    "            _, action = torch.max(q_values, dim=1)     # wir nehmen den grÃ¶ssten Wert\n",
    "            action = int(action.item())\n",
    "\n",
    "        return action\n",
    "\n",
    "    @torch.no_grad()                    # Netwerk lernt nicht sondern gibt nur Resultate zurÃ¼ck\n",
    "    def play_step(\n",
    "            self,\n",
    "            net: nn.Module,             # The DQN network\n",
    "            epsilon: float = 0.0,       # value to determine likelihood of taking a random action\n",
    "                                        # Set to 0 to play after the training\n",
    "            device: str = \"cpu\",        # Current device\n",
    "        ) -> Tuple[float, bool]:\n",
    "        \"\"\"Carries out a single interaction step between the agent and the environment.\n",
    "        Returns:\n",
    "            reward, done\n",
    "        \"\"\"\n",
    "        action = self.get_action(net, epsilon, device)\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "        exp = Experience(self.state, action, reward, done, new_state)\n",
    "        self.replay_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if done:\n",
    "            self.reset()\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb3c7e",
   "metadata": {
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.03211,
     "end_time": "2021-12-04T16:49:32.116593",
     "exception": false,
     "start_time": "2021-12-04T16:49:32.084483",
     "status": "completed"
    },
    "tags": [],
    "id": "17cb3c7e"
   },
   "source": [
    "# 5. DQN Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed5b8877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T16:49:32.185758Z",
     "iopub.status.busy": "2021-12-04T16:49:32.175032Z",
     "iopub.status.idle": "2021-12-04T16:49:32.190998Z",
     "shell.execute_reply": "2021-12-04T16:49:32.190598Z"
    },
    "papermill": {
     "duration": 0.045916,
     "end_time": "2021-12-04T16:49:32.191102",
     "exception": false,
     "start_time": "2021-12-04T16:49:32.145186",
     "status": "completed"
    },
    "tags": [],
    "id": "ed5b8877"
   },
   "outputs": [],
   "source": [
    "class DQNLightning(LightningModule):\n",
    "    \"\"\"Basic DQN Model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int         = 20,               # size of the batches (Test mit 32)\n",
    "        lr: float               = 0.001,            # learning rate\n",
    "        env: str                = \"CartPole-v0\",    # gym environment tag\n",
    "        gamma: float            = 0.95,             # discount factor\n",
    "        sync_rate: int          = 20,               # Aktualisierung der Gewichte\n",
    "        replay_size: int        = 1000000,          # capacity of the replay buffer (Test mit 10000)\n",
    "        warm_start_size: int    = 1000000,          # number of samples to fill our buffer at the start (Test mit 10_000)\n",
    "        eps_last_frame: int     = 1000,             # what frame should epsilon stop decaying (Test mit 100_000?)\n",
    "        eps_start: float        = 1.0,              # starting value of epsilon\n",
    "        eps_end: float          = 0.01,             # final value of epsilon\n",
    "        episode_length: int     = 1000,             # max length of an episode (Test mit 400)\n",
    "        warm_start_steps: int   = 1000000,          # max episode reward in the environment\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.env = gym.make(self.hparams.env)\n",
    "        obs_size = self.env.observation_space.shape[0]\n",
    "        n_actions = self.env.action_space.n\n",
    "\n",
    "        self.net = DQN(obs_size, n_actions)         # online network (wird trainiert)\n",
    "        self.target_net = DQN(obs_size, n_actions)  # target network\n",
    "\n",
    "        self.buffer = ReplayBuffer(self.hparams.replay_size)\n",
    "        self.agent = Agent(self.env, self.buffer)\n",
    "        self.total_reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.populate(self.hparams.warm_start_steps)\n",
    "\n",
    "    def populate(self, steps: int = 1000) -> None:\n",
    "        \"\"\"Carries out several random steps through the environment to initially\n",
    "        fill up the replay buffer with experiences.\n",
    "\n",
    "        Args:\n",
    "            steps: number of random steps to populate the buffer with\n",
    "        \"\"\"\n",
    "        for i in range(steps):\n",
    "            self.agent.play_step(self.net, epsilon=1.0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "\n",
    "        Args:\n",
    "            x: environment state\n",
    "\n",
    "        Returns:\n",
    "            q values\n",
    "        \"\"\"\n",
    "        output = self.net(x)\n",
    "        return output               # Output sind Q-Values\n",
    "\n",
    "    def dqn_mse_loss(self, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        \"\"\"Calculates the mse loss using a mini batch from the replay buffer.\n",
    "        Args:  batch: current mini batch of replay data\n",
    "        Returns: loss\n",
    "        \"\"\"\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "        # q-values holen (current)\n",
    "        state_action_values = self.net(states).gather(1, actions.type(torch.int64).unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # next holen aus targen\n",
    "            next_state_values = self.target_net(next_states).max(1)[0]\n",
    "            next_state_values[dones] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "\n",
    "        # dito SARSA max: target = reward + (gamma * Qsa_next)\n",
    "        expected_state_action_values = next_state_values * self.hparams.gamma + rewards\n",
    "\n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], nb_batch) -> OrderedDict:\n",
    "        \"\"\"Carries out a single step through the environment to update the replay buffer.\n",
    "        Then calculates loss based on the minibatch recieved.\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "            nb_batch: batch number\n",
    "        Returns:\n",
    "            Training loss and log metrics\n",
    "        \"\"\"\n",
    "        device = self.get_device(batch)\n",
    "        epsilon = max(\n",
    "            self.hparams.eps_end,\n",
    "            self.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame,\n",
    "        )\n",
    "\n",
    "        # step through environment with agent\n",
    "        reward, done = self.agent.play_step(self.net, epsilon, device)\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        # calculates training loss\n",
    "        loss = self.dqn_mse_loss(batch)\n",
    "\n",
    "        # Bachward propagation step, done be lightning out of the box\n",
    "\n",
    "        # Multi GPU Training\n",
    "        if self.trainer._distrib_type in {DistributedType.DP, DistributedType.DDP2}:\n",
    "            loss = loss.unsqueeze(0)\n",
    "\n",
    "        if done:\n",
    "            self.total_reward = self.episode_reward\n",
    "            self.episode_reward = 0\n",
    "\n",
    "        # Soft update of target network\n",
    "        if self.global_step % self.hparams.sync_rate == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "        log = {\n",
    "            \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "            \"reward\": torch.tensor(reward).to(device),\n",
    "            \"train_loss\": loss,\n",
    "        }\n",
    "        status = {\n",
    "            \"steps\": torch.tensor(self.global_step).to(device),\n",
    "            \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "        }\n",
    "\n",
    "        return OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": status})\n",
    "\n",
    "    def configure_optimizers(self) -> List[Optimizer]:\n",
    "        \"\"\"Initialize Adam optimizer.\"\"\"\n",
    "        optimizer = Adam(self.net.parameters(), lr=self.hparams.lr)\n",
    "        return [optimizer]\n",
    "\n",
    "    def __dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "        dataset = RLDataset(self.buffer, self.hparams.episode_length)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get train loader.\"\"\"\n",
    "        return self.__dataloader()\n",
    "\n",
    "    def get_device(self, batch) -> str:\n",
    "        \"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
    "        return batch[0].device.index if self.on_gpu else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068b50b",
   "metadata": {
    "papermill": {
     "duration": 0.029024,
     "end_time": "2021-12-04T16:49:32.248526",
     "exception": false,
     "start_time": "2021-12-04T16:49:32.219502",
     "status": "completed"
    },
    "tags": [],
    "id": "6068b50b"
   },
   "source": [
    "# 6. Trainer\n",
    "\n",
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "223c17c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T16:49:32.308551Z",
     "iopub.status.busy": "2021-12-04T16:49:32.308085Z",
     "iopub.status.idle": "2021-12-04T16:49:48.709258Z",
     "shell.execute_reply": "2021-12-04T16:49:48.709649Z"
    },
    "papermill": {
     "duration": 16.433171,
     "end_time": "2021-12-04T16:49:48.709816",
     "exception": false,
     "start_time": "2021-12-04T16:49:32.276645",
     "status": "completed"
    },
    "tags": [],
    "id": "223c17c5",
    "outputId": "b254934b-4acb-4245-be15-185cfe325312",
    "colab": {
     "referenced_widgets": [
      "7809a9f0b92743a3bdfcc9d4e6096fec"
     ]
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name       | Type | Params\n",
      "------------------------------------\n",
      "0 | net        | DQN  | 770   \n",
      "1 | target_net | DQN  | 770   \n",
      "------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "D:\\dev\\software\\miniconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d652196d74bf4c6faf10e7d3853a35e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev\\software\\miniconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\closure.py:35: LightningDeprecationWarning: One of the returned values {'log', 'progress_bar'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "model = DQNLightning()\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=AVAIL_GPUS,\n",
    "    max_epochs=1000,\n",
    "    val_check_interval=100,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show Tensorboard (disabled)\n",
    "\n",
    "Run in Terminal with:\n",
    "```\n",
    "tensorboard --logdir ligthning_logs\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0dc7a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T16:49:48.784401Z",
     "iopub.status.busy": "2021-12-04T16:49:48.783938Z",
     "iopub.status.idle": "2021-12-04T16:49:50.361154Z",
     "shell.execute_reply": "2021-12-04T16:49:50.361542Z"
    },
    "papermill": {
     "duration": 1.615711,
     "end_time": "2021-12-04T16:49:50.361705",
     "exception": false,
     "start_time": "2021-12-04T16:49:48.745994",
     "status": "completed"
    },
    "tags": [],
    "id": "b0dc7a72",
    "outputId": "0f5930f6-715a-4eb3-f6d4-62599cd3509b"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Test Score\n",
    "\n",
    "## Play a single game"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of a single game:\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "def play_game():\n",
    "    agent = model.agent\n",
    "    agent.env.reset()\n",
    "    score = 0\n",
    "    for t in range(1000):\n",
    "        score = t+1\n",
    "        action, _  = agent.play_step(net=model.net, epsilon=0, device='cpu')\n",
    "        state, reward, done, _ = agent.env.step(int(action))\n",
    "        if done:\n",
    "            break\n",
    "    return score\n",
    "\n",
    "score = play_game()\n",
    "print(\"Score of a single game:\")\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Play n games"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score of 1000 games:\n",
      "11.596\n"
     ]
    }
   ],
   "source": [
    "def play_games(n):\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += play_game()\n",
    "    return total/n\n",
    "\n",
    "n = 1000\n",
    "avg_score = play_games(n)\n",
    "print(\"Average score of \" + str(n) + \" games:\")\n",
    "print(avg_score)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Visualize the game\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plotting options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.pyplot._IonContext at 0x12a21ec4280>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "state = model.agent.env.reset()\n",
    "img = plt.imshow(model.agent.env.render(mode='rgb_array'))\n",
    "for t in range(1000):\n",
    "    # step through environment with agent\n",
    "    action,_ = model.agent.play_step(model.net, 0, 'cpu')\n",
    "    #action = env.action_space.sample()\n",
    "    img.set_data(model.agent.env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = model.agent.env.step(int(action))\n",
    "    if done:\n",
    "        print('Score: ', t+1)\n",
    "        break\n",
    "\n",
    "model.agent.env.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "__The end.__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "id,colab_type,colab,-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.486464,
   "end_time": "2021-12-04T16:49:52.782807",
   "environment_variables": {},
   "exception": null,
   "input_path": "lightning_examples/reinforce-learning-DQN/dqn.ipynb",
   "output_path": ".notebooks/lightning_examples/reinforce-learning-DQN.ipynb",
   "parameters": {},
   "start_time": "2021-12-04T16:49:23.296343",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  },
  "colab": {
   "name": "reinforce-learning-DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "pycharm-ec18b53a",
   "language": "python",
   "display_name": "PyCharm (python-work)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}