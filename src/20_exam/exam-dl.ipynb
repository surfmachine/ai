{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exam Preparation Deep Learning\n",
    "18.03.2022, Thomas Iten\n",
    "\n",
    "**Content**\n",
    "1. Dense Network\n",
    "2. AutoEncoder\n",
    "3. CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Dense Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "#\n",
    "# Setup device (cuda / cpu)\n",
    "#\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Device:\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Download data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor())                       # ToTensor() erstellt proprietäres Format für PyTorch\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Explore data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Train Data\n",
      "#\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "\n",
      "#\n",
      "# Train Data Classes\n",
      "#\n",
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
      "\n",
      "\n",
      "#\n",
      "# Train Data ClassToIndex\n",
      "#\n",
      "{'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9}\n"
     ]
    }
   ],
   "source": [
    "print(\"#\")\n",
    "print(\"# Train Data\")\n",
    "print(\"#\")\n",
    "print(train_data)\n",
    "\n",
    "print(\"\\n#\")\n",
    "print(\"# Train Data Classes\")\n",
    "print(\"#\")\n",
    "print(train_data.classes)\n",
    "print()\n",
    "\n",
    "print(\"\\n#\")\n",
    "print(\"# Train Data ClassToIndex\")\n",
    "print(\"#\")\n",
    "print(train_data.class_to_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Create Data Loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader  = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 Neuronal Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=10, bias=True)\n",
      "    (4): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(     # Einfaches lineares/sequentielles Netzwerk, 2 Layer\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 128),                  # Input=28x28 / Output=128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,10),                      # Input=128   / Output = 10\n",
    "            nn.Softmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)                  # Angabe wo das ausgeführt werden soll\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 Loss und Optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Backward\n",
    "# - Definition Backward Loss Funktion\n",
    "# - Build partial derivates of loss function and grads\n",
    "loss_fn = nn.CrossEntropyLoss() # MSE\n",
    "\n",
    "# Step\n",
    "# - Optimizer aktualisiert die Gewichte\n",
    "# - Der optimizer macht den Update step intelligenter\n",
    "# - Update: params = params - learing_rate * grad\n",
    "# - Besser ist Adam optimizer\n",
    "#   optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.7 Train und Test Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):     # Mit enumerate erhält man batch nummer\n",
    "        X, y = X.to(device), y.to(device)           # je Bacht Bilddaten in X, Label in Y\n",
    "\n",
    "        #\n",
    "        # Compute prediction error\n",
    "        #\n",
    "        pred = model(X)                             # Impliziter Aufruf von model.forward\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #\n",
    "        # Backpropagation\n",
    "        #\n",
    "        optimizer.zero_grad()                       # Reset Anfangswerte für jeden Batch\n",
    "\n",
    "        loss.backward()                             # X.size Ableitungen und Werte\n",
    "                                                    # 1. Partielle Ableitungen bilden\n",
    "                                                    # 2. Werte werden in Ableitungsformel ein geplugged\n",
    "                                                    #    X Werte, Y Werte, Gewichte und Bias\n",
    "                                                    #    Pro Ableitung erhlät man einen Skalar (m_deriv)\n",
    "                                                    #    Ein Gradient pro Feature und Bias\n",
    "\n",
    "        optimizer.step()                            # Update der Gradienten (konkrete Werte der Ableitung)\n",
    "                                                    # Gewichte/Bias = Gewichte/Bias bisher - LR * Gradienten\n",
    "\n",
    "        #\n",
    "        # Zwischenresultate zeigen\n",
    "        #\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X) # batch * len(X) = Position Build\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():                           # no_grad: keine Aufzeichnung, nur Prediction\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()    # item() holt den Wert aus dem Objekt\n",
    "\n",
    "                                                    # argmax holt grösste von den 10 Kategorien\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.8 Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      "-------------------------------\n",
      "loss: 2.300862  [    0/60000]\n",
      "loss: 1.825657  [ 6400/60000]\n",
      "loss: 1.655431  [12800/60000]\n",
      "loss: 1.753543  [19200/60000]\n",
      "loss: 1.698539  [25600/60000]\n",
      "loss: 1.667875  [32000/60000]\n",
      "loss: 1.641326  [38400/60000]\n",
      "loss: 1.669391  [44800/60000]\n",
      "loss: 1.633124  [51200/60000]\n",
      "loss: 1.666724  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 1.639786 \n",
      "\n",
      "Epoch: 2 \n",
      "-------------------------------\n",
      "loss: 1.599909  [    0/60000]\n",
      "loss: 1.635655  [ 6400/60000]\n",
      "loss: 1.604491  [12800/60000]\n",
      "loss: 1.668910  [19200/60000]\n",
      "loss: 1.651608  [25600/60000]\n",
      "loss: 1.663676  [32000/60000]\n",
      "loss: 1.590811  [38400/60000]\n",
      "loss: 1.657717  [44800/60000]\n",
      "loss: 1.634533  [51200/60000]\n",
      "loss: 1.644574  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 1.627879 \n",
      "\n",
      "Epoch: 3 \n",
      "-------------------------------\n",
      "loss: 1.590475  [    0/60000]\n",
      "loss: 1.615870  [ 6400/60000]\n",
      "loss: 1.590615  [12800/60000]\n",
      "loss: 1.656596  [19200/60000]\n",
      "loss: 1.629151  [25600/60000]\n",
      "loss: 1.633142  [32000/60000]\n",
      "loss: 1.593609  [38400/60000]\n",
      "loss: 1.661536  [44800/60000]\n",
      "loss: 1.654360  [51200/60000]\n",
      "loss: 1.616737  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 1.622114 \n",
      "\n",
      "Epoch: 4 \n",
      "-------------------------------\n",
      "loss: 1.576447  [    0/60000]\n",
      "loss: 1.585409  [ 6400/60000]\n",
      "loss: 1.591445  [12800/60000]\n",
      "loss: 1.649798  [19200/60000]\n",
      "loss: 1.612325  [25600/60000]\n",
      "loss: 1.637341  [32000/60000]\n",
      "loss: 1.574004  [38400/60000]\n",
      "loss: 1.660561  [44800/60000]\n",
      "loss: 1.629240  [51200/60000]\n",
      "loss: 1.628912  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 1.617724 \n",
      "\n",
      "Epoch: 5 \n",
      "-------------------------------\n",
      "loss: 1.585458  [    0/60000]\n",
      "loss: 1.580275  [ 6400/60000]\n",
      "loss: 1.590629  [12800/60000]\n",
      "loss: 1.641105  [19200/60000]\n",
      "loss: 1.581399  [25600/60000]\n",
      "loss: 1.636809  [32000/60000]\n",
      "loss: 1.576660  [38400/60000]\n",
      "loss: 1.647982  [44800/60000]\n",
      "loss: 1.633547  [51200/60000]\n",
      "loss: 1.621412  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 1.619938 \n",
      "\n",
      "Epoch: 6 \n",
      "-------------------------------\n",
      "loss: 1.590580  [    0/60000]\n",
      "loss: 1.571777  [ 6400/60000]\n",
      "loss: 1.578534  [12800/60000]\n",
      "loss: 1.621214  [19200/60000]\n",
      "loss: 1.577331  [25600/60000]\n",
      "loss: 1.627005  [32000/60000]\n",
      "loss: 1.554432  [38400/60000]\n",
      "loss: 1.648613  [44800/60000]\n",
      "loss: 1.616729  [51200/60000]\n",
      "loss: 1.606400  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 1.621124 \n",
      "\n",
      "Epoch: 7 \n",
      "-------------------------------\n",
      "loss: 1.579431  [    0/60000]\n",
      "loss: 1.582810  [ 6400/60000]\n",
      "loss: 1.576222  [12800/60000]\n",
      "loss: 1.631325  [19200/60000]\n",
      "loss: 1.587110  [25600/60000]\n",
      "loss: 1.604071  [32000/60000]\n",
      "loss: 1.581053  [38400/60000]\n",
      "loss: 1.619962  [44800/60000]\n",
      "loss: 1.616453  [51200/60000]\n",
      "loss: 1.573639  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 1.611637 \n",
      "\n",
      "Epoch: 8 \n",
      "-------------------------------\n",
      "loss: 1.580504  [    0/60000]\n",
      "loss: 1.574700  [ 6400/60000]\n",
      "loss: 1.571623  [12800/60000]\n",
      "loss: 1.646870  [19200/60000]\n",
      "loss: 1.607367  [25600/60000]\n",
      "loss: 1.621734  [32000/60000]\n",
      "loss: 1.559376  [38400/60000]\n",
      "loss: 1.625730  [44800/60000]\n",
      "loss: 1.609965  [51200/60000]\n",
      "loss: 1.601864  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 1.607839 \n",
      "\n",
      "Epoch: 9 \n",
      "-------------------------------\n",
      "loss: 1.574822  [    0/60000]\n",
      "loss: 1.579475  [ 6400/60000]\n",
      "loss: 1.575798  [12800/60000]\n",
      "loss: 1.646143  [19200/60000]\n",
      "loss: 1.604874  [25600/60000]\n",
      "loss: 1.607003  [32000/60000]\n",
      "loss: 1.563865  [38400/60000]\n",
      "loss: 1.638716  [44800/60000]\n",
      "loss: 1.611623  [51200/60000]\n",
      "loss: 1.611446  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 1.603117 \n",
      "\n",
      "Epoch: 10 \n",
      "-------------------------------\n",
      "loss: 1.569262  [    0/60000]\n",
      "loss: 1.575222  [ 6400/60000]\n",
      "loss: 1.577450  [12800/60000]\n",
      "loss: 1.658138  [19200/60000]\n",
      "loss: 1.608474  [25600/60000]\n",
      "loss: 1.631923  [32000/60000]\n",
      "loss: 1.559278  [38400/60000]\n",
      "loss: 1.620399  [44800/60000]\n",
      "loss: 1.594813  [51200/60000]\n",
      "loss: 1.584557  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 1.599448 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader  = DataLoader(test_data,  batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch+1, \"\\n-------------------------------\")\n",
    "    train(dataloader=train_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    test( dataloader=test_dataloader,  model=model, loss_fn=loss_fn)\n",
    "    # train(train_dataloader, model, loss_fn, optimizer)\n",
    "    # test (test_dataloader,  model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. AutoEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}