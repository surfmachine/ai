{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exam Preparation Deep Learning\n",
    "18.03.2022, Thomas Iten"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. AutoEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "#\n",
    "# Setup device (cuda / cpu)\n",
    "#\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Device:\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Download data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor())                       # ToTensor() erstellt proprietäres Format für PyTorch\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Explore data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Train Data\n",
      "#\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "\n",
      "#\n",
      "# Train Data Classes\n",
      "#\n",
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
      "\n",
      "\n",
      "#\n",
      "# Train Data ClassToIndex\n",
      "#\n",
      "{'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9}\n"
     ]
    }
   ],
   "source": [
    "print(\"#\")\n",
    "print(\"# Train Data\")\n",
    "print(\"#\")\n",
    "print(train_data)\n",
    "\n",
    "print(\"\\n#\")\n",
    "print(\"# Train Data Classes\")\n",
    "print(\"#\")\n",
    "print(train_data.classes)\n",
    "print()\n",
    "\n",
    "print(\"\\n#\")\n",
    "print(\"# Train Data ClassToIndex\")\n",
    "print(\"#\")\n",
    "print(train_data.class_to_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Create Data Loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader  = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 AutoEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=36, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=36, out_features=18, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=18, out_features=9, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=36, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=36, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=128, out_features=784, bias=True)\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    \"\"\"AutoEncoder with:  28*28 (784) >> 9 >> 28*28\"\"\"\n",
    "\n",
    "    def __init__(self, n_features=28 * 28):\n",
    "        super().__init__()\n",
    "\n",
    "        # Building an linear encoder with Linear layer followed by Relu activation function: 28*28 >> 9\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_features, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 9)\n",
    "        )\n",
    "\n",
    "        # Building an linear decoder with Linear layer followed by Relu activation function: 9 >> 28*28\n",
    "        # The Sigmoid activation function outputs the value between 0 and 1\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(9, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, n_features),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "model = AutoEncoder().to(device)                  # Angabe wo das ausgeführt werden soll\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6 Loss und Optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Backward\n",
    "# - Definition Backward Loss Funktion\n",
    "# - Build partial derivates of loss function and grads\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Step\n",
    "# - Optimizer aktualisiert die Gewichte\n",
    "# - Der optimizer macht den Update step intelligenter\n",
    "# - Update: params = params - learing_rate * grad\n",
    "# - Besser ist Adam optimizer\n",
    "#   optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.7 Train und Test Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):     # Mit enumerate erhält man batch nummer\n",
    "        #\n",
    "        # Reshape\n",
    "        #\n",
    "        X = X.reshape(-1, 28 * 28)                  # reshape n_features 28*28\n",
    "        X = X.to(device)\n",
    "\n",
    "        #\n",
    "        # Compute prediction error\n",
    "        #\n",
    "        pred = model(X)                             # Impliziter Aufruf von model.forward\n",
    "        loss = loss_fn(pred, X)\n",
    "\n",
    "        #\n",
    "        # Backpropagation\n",
    "        #\n",
    "        optimizer.zero_grad()                       # Reset Anfangswerte für jeden Batch\n",
    "\n",
    "        loss.backward()                             # X.size Ableitungen und Werte\n",
    "                                                    # 1. Partielle Ableitungen bilden\n",
    "                                                    # 2. Werte werden in Ableitungsformel ein geplugged\n",
    "                                                    #    X Werte, Y Werte, Gewichte und Bias\n",
    "                                                    #    Pro Ableitung erhlät man einen Skalar (m_deriv)\n",
    "                                                    #    Ein Gradient pro Feature und Bias\n",
    "\n",
    "        optimizer.step()                            # Update der Gradienten (konkrete Werte der Ableitung)\n",
    "                                                    # Gewichte/Bias = Gewichte/Bias bisher - LR * Gradienten\n",
    "\n",
    "        #\n",
    "        # Zwischenresultate zeigen\n",
    "        #\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X) # batch * len(X) = Position Build\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():                           # no_grad: keine Aufzeichnung, nur Prediction\n",
    "        for X, y in dataloader:\n",
    "            X = X.reshape(-1, 28 * 28)              # reshape n_features 28*28\n",
    "            X = X.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, X).item()    # item() holt den Wert aus dem Objekt\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.8 Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      "-------------------------------\n",
      "loss: 0.171169  [    0/60000]\n",
      "loss: 0.063880  [ 6400/60000]\n",
      "loss: 0.057043  [12800/60000]\n",
      "loss: 0.040360  [19200/60000]\n",
      "loss: 0.041132  [25600/60000]\n",
      "loss: 0.037380  [32000/60000]\n",
      "loss: 0.030040  [38400/60000]\n",
      "loss: 0.030992  [44800/60000]\n",
      "loss: 0.030865  [51200/60000]\n",
      "loss: 0.029687  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 0.031179 \n",
      "\n",
      "Epoch: 2 \n",
      "-------------------------------\n",
      "loss: 0.030771  [    0/60000]\n",
      "loss: 0.027877  [ 6400/60000]\n",
      "loss: 0.027732  [12800/60000]\n",
      "loss: 0.026731  [19200/60000]\n",
      "loss: 0.028001  [25600/60000]\n",
      "loss: 0.029044  [32000/60000]\n",
      "loss: 0.025225  [38400/60000]\n",
      "loss: 0.025744  [44800/60000]\n",
      "loss: 0.026972  [51200/60000]\n",
      "loss: 0.025182  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 0.026938 \n",
      "\n",
      "Epoch: 3 \n",
      "-------------------------------\n",
      "loss: 0.027602  [    0/60000]\n",
      "loss: 0.026073  [ 6400/60000]\n",
      "loss: 0.024830  [12800/60000]\n",
      "loss: 0.026350  [19200/60000]\n",
      "loss: 0.025740  [25600/60000]\n",
      "loss: 0.026849  [32000/60000]\n",
      "loss: 0.023861  [38400/60000]\n",
      "loss: 0.024503  [44800/60000]\n",
      "loss: 0.025629  [51200/60000]\n",
      "loss: 0.023656  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 0.025687 \n",
      "\n",
      "Epoch: 4 \n",
      "-------------------------------\n",
      "loss: 0.027265  [    0/60000]\n",
      "loss: 0.023370  [ 6400/60000]\n",
      "loss: 0.023229  [12800/60000]\n",
      "loss: 0.023778  [19200/60000]\n",
      "loss: 0.024738  [25600/60000]\n",
      "loss: 0.025395  [32000/60000]\n",
      "loss: 0.023330  [38400/60000]\n",
      "loss: 0.022489  [44800/60000]\n",
      "loss: 0.024043  [51200/60000]\n",
      "loss: 0.022601  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 0.023688 \n",
      "\n",
      "Epoch: 5 \n",
      "-------------------------------\n",
      "loss: 0.024536  [    0/60000]\n",
      "loss: 0.022239  [ 6400/60000]\n",
      "loss: 0.021250  [12800/60000]\n",
      "loss: 0.022561  [19200/60000]\n",
      "loss: 0.023541  [25600/60000]\n",
      "loss: 0.023243  [32000/60000]\n",
      "loss: 0.021910  [38400/60000]\n",
      "loss: 0.020852  [44800/60000]\n",
      "loss: 0.022662  [51200/60000]\n",
      "loss: 0.021892  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 0.022724 \n",
      "\n",
      "Epoch: 6 \n",
      "-------------------------------\n",
      "loss: 0.022997  [    0/60000]\n",
      "loss: 0.021574  [ 6400/60000]\n",
      "loss: 0.020819  [12800/60000]\n",
      "loss: 0.022046  [19200/60000]\n",
      "loss: 0.022814  [25600/60000]\n",
      "loss: 0.022329  [32000/60000]\n",
      "loss: 0.021045  [38400/60000]\n",
      "loss: 0.020040  [44800/60000]\n",
      "loss: 0.021698  [51200/60000]\n",
      "loss: 0.021348  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 0.021597 \n",
      "\n",
      "Epoch: 7 \n",
      "-------------------------------\n",
      "loss: 0.021952  [    0/60000]\n",
      "loss: 0.020971  [ 6400/60000]\n",
      "loss: 0.020245  [12800/60000]\n",
      "loss: 0.020848  [19200/60000]\n",
      "loss: 0.020900  [25600/60000]\n",
      "loss: 0.021565  [32000/60000]\n",
      "loss: 0.019977  [38400/60000]\n",
      "loss: 0.018425  [44800/60000]\n",
      "loss: 0.020655  [51200/60000]\n",
      "loss: 0.020354  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 0.020667 \n",
      "\n",
      "Epoch: 8 \n",
      "-------------------------------\n",
      "loss: 0.020851  [    0/60000]\n",
      "loss: 0.020045  [ 6400/60000]\n",
      "loss: 0.019448  [12800/60000]\n",
      "loss: 0.020003  [19200/60000]\n",
      "loss: 0.020359  [25600/60000]\n",
      "loss: 0.021170  [32000/60000]\n",
      "loss: 0.019346  [38400/60000]\n",
      "loss: 0.017761  [44800/60000]\n",
      "loss: 0.019797  [51200/60000]\n",
      "loss: 0.019857  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 0.019990 \n",
      "\n",
      "Epoch: 9 \n",
      "-------------------------------\n",
      "loss: 0.020147  [    0/60000]\n",
      "loss: 0.019515  [ 6400/60000]\n",
      "loss: 0.018727  [12800/60000]\n",
      "loss: 0.020077  [19200/60000]\n",
      "loss: 0.019781  [25600/60000]\n",
      "loss: 0.020869  [32000/60000]\n",
      "loss: 0.018949  [38400/60000]\n",
      "loss: 0.017468  [44800/60000]\n",
      "loss: 0.019407  [51200/60000]\n",
      "loss: 0.019597  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 0.019796 \n",
      "\n",
      "Epoch: 10 \n",
      "-------------------------------\n",
      "loss: 0.019875  [    0/60000]\n",
      "loss: 0.019271  [ 6400/60000]\n",
      "loss: 0.018192  [12800/60000]\n",
      "loss: 0.019667  [19200/60000]\n",
      "loss: 0.019423  [25600/60000]\n",
      "loss: 0.020398  [32000/60000]\n",
      "loss: 0.018679  [38400/60000]\n",
      "loss: 0.017136  [44800/60000]\n",
      "loss: 0.019228  [51200/60000]\n",
      "loss: 0.019341  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 0.019493 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 # 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch+1, \"\\n-------------------------------\")\n",
    "    train(dataloader=train_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    test( dataloader=test_dataloader,  model=model, loss_fn=loss_fn)\n",
    "    # save weigths\n",
    "    # torch.save(model.state_dict(), filename=f\"model_epoch_{epoch}.pth\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.9 Test images with noise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      "-------------------------------\n",
      "loss: 0.019624  [    0/60000]\n",
      "loss: 0.019065  [ 6400/60000]\n",
      "loss: 0.017773  [12800/60000]\n",
      "loss: 0.019153  [19200/60000]\n",
      "loss: 0.019114  [25600/60000]\n",
      "loss: 0.020253  [32000/60000]\n",
      "loss: 0.018500  [38400/60000]\n",
      "loss: 0.016867  [44800/60000]\n",
      "loss: 0.018842  [51200/60000]\n",
      "loss: 0.019182  [57600/60000]\n",
      "Epoch: 2 \n",
      "-------------------------------\n",
      "loss: 0.019294  [    0/60000]\n",
      "loss: 0.018893  [ 6400/60000]\n",
      "loss: 0.017339  [12800/60000]\n",
      "loss: 0.019122  [19200/60000]\n",
      "loss: 0.018663  [25600/60000]\n",
      "loss: 0.020012  [32000/60000]\n",
      "loss: 0.018484  [38400/60000]\n",
      "loss: 0.016574  [44800/60000]\n",
      "loss: 0.018603  [51200/60000]\n",
      "loss: 0.018978  [57600/60000]\n",
      "Epoch: 3 \n",
      "-------------------------------\n",
      "loss: 0.019105  [    0/60000]\n",
      "loss: 0.018714  [ 6400/60000]\n",
      "loss: 0.016996  [12800/60000]\n",
      "loss: 0.018815  [19200/60000]\n",
      "loss: 0.018600  [25600/60000]\n",
      "loss: 0.019764  [32000/60000]\n",
      "loss: 0.018125  [38400/60000]\n",
      "loss: 0.016358  [44800/60000]\n",
      "loss: 0.018792  [51200/60000]\n",
      "loss: 0.018838  [57600/60000]\n",
      "Epoch: 4 \n",
      "-------------------------------\n",
      "loss: 0.018775  [    0/60000]\n",
      "loss: 0.018642  [ 6400/60000]\n",
      "loss: 0.016714  [12800/60000]\n",
      "loss: 0.018680  [19200/60000]\n",
      "loss: 0.018514  [25600/60000]\n",
      "loss: 0.019584  [32000/60000]\n",
      "loss: 0.017944  [38400/60000]\n",
      "loss: 0.016263  [44800/60000]\n",
      "loss: 0.018276  [51200/60000]\n",
      "loss: 0.018817  [57600/60000]\n",
      "Epoch: 5 \n",
      "-------------------------------\n",
      "loss: 0.018546  [    0/60000]\n",
      "loss: 0.018518  [ 6400/60000]\n",
      "loss: 0.016440  [12800/60000]\n",
      "loss: 0.018419  [19200/60000]\n",
      "loss: 0.018457  [25600/60000]\n",
      "loss: 0.019335  [32000/60000]\n",
      "loss: 0.017822  [38400/60000]\n",
      "loss: 0.016151  [44800/60000]\n",
      "loss: 0.018067  [51200/60000]\n",
      "loss: 0.018663  [57600/60000]\n",
      "Epoch: 6 \n",
      "-------------------------------\n",
      "loss: 0.018405  [    0/60000]\n",
      "loss: 0.018399  [ 6400/60000]\n",
      "loss: 0.016291  [12800/60000]\n",
      "loss: 0.018293  [19200/60000]\n",
      "loss: 0.018364  [25600/60000]\n",
      "loss: 0.019112  [32000/60000]\n",
      "loss: 0.017719  [38400/60000]\n",
      "loss: 0.016047  [44800/60000]\n",
      "loss: 0.017996  [51200/60000]\n",
      "loss: 0.018568  [57600/60000]\n",
      "Epoch: 7 \n",
      "-------------------------------\n",
      "loss: 0.018217  [    0/60000]\n",
      "loss: 0.018530  [ 6400/60000]\n",
      "loss: 0.016075  [12800/60000]\n",
      "loss: 0.018122  [19200/60000]\n",
      "loss: 0.018228  [25600/60000]\n",
      "loss: 0.018894  [32000/60000]\n",
      "loss: 0.017460  [38400/60000]\n",
      "loss: 0.015913  [44800/60000]\n",
      "loss: 0.017907  [51200/60000]\n",
      "loss: 0.018460  [57600/60000]\n",
      "Epoch: 8 \n",
      "-------------------------------\n",
      "loss: 0.018117  [    0/60000]\n",
      "loss: 0.018239  [ 6400/60000]\n",
      "loss: 0.015959  [12800/60000]\n",
      "loss: 0.017963  [19200/60000]\n",
      "loss: 0.018059  [25600/60000]\n",
      "loss: 0.018771  [32000/60000]\n",
      "loss: 0.017332  [38400/60000]\n",
      "loss: 0.015815  [44800/60000]\n",
      "loss: 0.017569  [51200/60000]\n",
      "loss: 0.018313  [57600/60000]\n",
      "Epoch: 9 \n",
      "-------------------------------\n",
      "loss: 0.017869  [    0/60000]\n",
      "loss: 0.017988  [ 6400/60000]\n",
      "loss: 0.015769  [12800/60000]\n",
      "loss: 0.017653  [19200/60000]\n",
      "loss: 0.017749  [25600/60000]\n",
      "loss: 0.018538  [32000/60000]\n",
      "loss: 0.017030  [38400/60000]\n",
      "loss: 0.015660  [44800/60000]\n",
      "loss: 0.017136  [51200/60000]\n",
      "loss: 0.018014  [57600/60000]\n",
      "Epoch: 10 \n",
      "-------------------------------\n",
      "loss: 0.017428  [    0/60000]\n",
      "loss: 0.017680  [ 6400/60000]\n",
      "loss: 0.015500  [12800/60000]\n",
      "loss: 0.017570  [19200/60000]\n",
      "loss: 0.017436  [25600/60000]\n",
      "loss: 0.018334  [32000/60000]\n",
      "loss: 0.016884  [38400/60000]\n",
      "loss: 0.015432  [44800/60000]\n",
      "loss: 0.016949  [51200/60000]\n",
      "loss: 0.017689  [57600/60000]\n"
     ]
    }
   ],
   "source": [
    "path='autoencoder/'\n",
    "\n",
    "def test_with_testimage(model, test_image, epoch, filename):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_image = test_image.reshape(-1, 28 * 28)\n",
    "        test_image = test_image.to(device)\n",
    "        predicted_image = model(test_image)\n",
    "        predicted_image = predicted_image.reshape(-1, 28, 28)\n",
    "        fn = path + f\"{filename}-epoch-{epoch}.png\"\n",
    "        save_image(predicted_image[0], fn)\n",
    "\n",
    "test_image = test_data.data[0, :, :].float()\n",
    "save_image(test_image, path + \"image.png\")\n",
    "\n",
    "noise = torch.randn(test_image.shape) * 0.1                     # Add more noise with 0.2, 0.3...\n",
    "noisy_test_image = torch.add(test_image, noise)\n",
    "save_image(noisy_test_image, path + \"image-noisy.png\")\n",
    "\n",
    "epochs = 10 # 100\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch+1, \"\\n-------------------------------\")\n",
    "    train(dataloader=train_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    test_with_testimage(model=model, test_image=test_image, epoch=epoch, filename=\"test\")\n",
    "    test_with_testimage(model=model, test_image=noisy_test_image, epoch=epoch, filename=\"test-noisy\")\n",
    "    # save weigths\n",
    "    # torch.save(model.state_dict(), filename=f\"model_epoch_{epoch}.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### _The end._\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}