{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "pycharm-ec18b53a",
   "language": "python",
   "display_name": "PyCharm (python-work)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "colab": {
   "name": "Monte_Carlo_Solution.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlgE6t7crBPH"
   },
   "source": [
    "# Aufgabe 10 - Monte Carlo Methods mit Frozen Lake\n",
    "15.01.2022, Thomas Iten\n",
    "\n",
    "\n",
    "**Content**\n",
    "0. Setup\n",
    "1. Create FrozenLakeHelper\n",
    "2. Explore Blackjack Environment\n",
    "3. Play random policy and testing policy\n",
    "4. Limit Stochastic Methode\n",
    "5. Monte Carlo Prediction\n",
    "6. Monte Carlo Control\n",
    "7. Monte Carlo Testing\n",
    "\n",
    "**References**\n",
    "- https://www.deep-teaching.org/notebooks/reinforcement-learning/exercise-monte-carlo-frozenlake-gym\n",
    "- https://adityajain.me/blogs/monte-carlo-and-temporal-difference.html\n",
    "- https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Setup\n",
    "\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TJM5HsNPrBPJ"
   },
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Create FrozenLakeHelper class\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FrozenLakeHelper():\n",
    "    \"\"\"Some helper methods used throughout this notebook.\"\"\"\n",
    "\n",
    "    def render(self, env, display_mode=\"brackets\", print_result=True, legend=False):\n",
    "        \"\"\"\n",
    "        IntelliJ notebooks to not render the color of the current position correct.\n",
    "        Details see: https://youtrack.jetbrains.com/issue/PY-32191\n",
    "\n",
    "        Therfore we use this customized render methode with two simple display modes.\n",
    "\n",
    "        :param env: The current environment to render it's fields.\n",
    "        :param display_mode: display current position with \"brackets\" or in \"lowercase\"\n",
    "        :param print_result: print the last action and result\n",
    "        :param legend: print the legend\n",
    "        :return: lastaction as text and fields with marked current position\n",
    "        \"\"\"\n",
    "\n",
    "        # init data\n",
    "        row, col = env.s // env.ncol, env.s % env.ncol\n",
    "        desc = env.desc.tolist()\n",
    "        desc = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
    "\n",
    "        actions = [\"Left\", \"Down\", \"Right\", \"Up\"]\n",
    "        action = \"Init\" if env.lastaction is None else actions[env.lastaction]\n",
    "\n",
    "        # format display mode\n",
    "        indicator = None\n",
    "        if display_mode == \"brackets\":\n",
    "            desc[row][col] = \"[{}]\".format(desc[row][col])\n",
    "            desc = [[ (\" {} \".format(c) if len(c) == 1 else c) for c in line ] for line in desc]\n",
    "            indicator = \"[]\"\n",
    "        elif display_mode == \"lowercase\":\n",
    "            desc[row][col] = (desc[row][col]).lower()\n",
    "            indicator = \"lowercase\"\n",
    "\n",
    "        # print result\n",
    "        if print_result:\n",
    "            if legend:\n",
    "                print(\"Last action:\", action)\n",
    "            else:\n",
    "                print(action + \":\")\n",
    "            for line in desc:\n",
    "                for pos in line:\n",
    "                    print(pos, end=\"\")\n",
    "                print(\"\")\n",
    "            if legend:\n",
    "                print(\"Legend: S=Start, F=Frozen (safe), H=Hole, G=Goal, \" + indicator + \"=Current Position\")\n",
    "            print(\"\")\n",
    "\n",
    "        # return result\n",
    "        return action, desc\n",
    "\n",
    "\n",
    "    def print_field_positions(self):\n",
    "        print(\"Field positons:\")\n",
    "        print(\"[ 0] [ 1] [ 2] [ 3]\")\n",
    "        print(\"[ 4] [ 5] [ 6] [ 7]\")\n",
    "        print(\"[ 8] [09] [10] [11]\")\n",
    "        print(\"[12] [13] [14] [15]\")\n",
    "\n",
    "    def print_actions(self):\n",
    "        print(\"Actions:\")\n",
    "        print(\"[0] Left\")\n",
    "        print(\"[1] Down\")\n",
    "        print(\"[2] Right\")\n",
    "        print(\"[3] Up\")\n",
    "\n",
    "    def print_Q(self, Q):\n",
    "        print(\"Field: Left        Down       Right      Up\")\n",
    "        for field in Q:\n",
    "            print(f\"{field : >5}\", end=\"\")\n",
    "            print(\":\", Q[field])\n",
    "\n",
    "# Create helper instance\n",
    "helper = FrozenLakeHelper()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Explore Frozen Lake Environment\n",
    "\n",
    "### Frozen Lake environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bzabrv8irBPK"
   },
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "print(\"Action space:\")\n",
    "print(env.action_space)\n",
    "print(\"\")\n",
    "\n",
    "helper.print_actions()\n",
    "print(\"\")\n",
    "\n",
    "print(\"Observation space:\")\n",
    "print(env.observation_space)\n",
    "print(\"\")\n",
    "\n",
    "helper.print_field_positions()\n"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space:\n",
      "Discrete(4)\n",
      "\n",
      "Actions:\n",
      "[0] Left\n",
      "[1] Down\n",
      "[2] Right\n",
      "[3] Up\n",
      "\n",
      "Observation space:\n",
      "Discrete(16)\n",
      "\n",
      "Field positons:\n",
      "[ 0] [ 1] [ 2] [ 3]\n",
      "[ 4] [ 5] [ 6] [ 7]\n",
      "[ 8] [09] [10] [11]\n",
      "[12] [13] [14] [15]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJiNsXfZrBPK"
   },
   "source": [
    "### Reset and initial state"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wV5DyGz2rBPK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c08c53d3-ea0d-440a-e22d-8571956163e1"
   },
   "source": [
    "env.reset()    # reset the environment the set agent to start state\n",
    "helper.render(env, legend=True)\n",
    "print()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last action: Init\n",
      "[S] F  F  F \n",
      " F  H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "Legend: S=Start, F=Frozen (safe), H=Hole, G=Goal, []=Current Position\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jz4CvL7rBPL"
   },
   "source": [
    "## 3. Play random policy and testing policy\n",
    "\n",
    "### Play random policy\n",
    "\n",
    "The step returns:\n",
    "- new_state: new state after action (random_action) taken in current state\n",
    "- reward: reward obtained after taken action (random_action) in current state and entering new state (new_state)\n",
    "- done: bool flag, true if goal or hole is reached\n",
    "- info: slippery probability, baseline is 1/3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "belzA6_PrBPL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b83c90c8-7842-45a0-cdf4-01f1381849d2"
   },
   "source": [
    "env.reset()\n",
    "helper.render(env)\n",
    "\n",
    "# Play till done or maximum 6 iterations reached\n",
    "for i in range(1,7):\n",
    "    print(\"Step:\", i)\n",
    "    random_action = env.action_space.sample() #samples a random action\n",
    "    new_state, reward, done, info = env.step(random_action) #agent takes action (random_action)\n",
    "    helper.render(env)\n",
    "    if done:\n",
    "        break"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init:\n",
      "[S] F  F  F \n",
      " F  H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "\n",
      "Step: 1\n",
      "Down:\n",
      " S  F  F  F \n",
      "[F] H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "\n",
      "Step: 2\n",
      "Left:\n",
      " S  F  F  F \n",
      "[F] H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "\n",
      "Step: 3\n",
      "Right:\n",
      " S  F  F  F \n",
      " F [H] F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing policy\n",
    "\n",
    "The problem is said to be solved, a good policy found, if over a sufficient number of episodes (>100)\n",
    "a mean reward of >0.7 is reached.\n",
    "\n",
    "The code below tests this for a given policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def test_performance(policy, nb_episodes=100):\n",
    "    sum_returns = 0\n",
    "    for i in range(nb_episodes):\n",
    "        state  = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                sum_returns += reward\n",
    "    return sum_returns/nb_episodes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets set and test a random policy. Since later we will work with aϵ−policy we always will start by defining\n",
    "a policy as a dictionary (state-action pairs) or array (index-value pairs) and than wrap the dictionary\n",
    "or array into a function.\n",
    "\n",
    "> We see that the number is far lower than 0.7 and therefor the policy is not good/ optimal."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test random policy:\n",
      "Mean reward = 0.01\n"
     ]
    }
   ],
   "source": [
    "# Define random policy\n",
    "random_policy = lambda s: env.action_space.sample()\n",
    "\n",
    "# test random policy\n",
    "mean_reward = test_performance(random_policy)\n",
    "print(\"Test random policy:\")\n",
    "print(\"Mean reward =\", mean_reward)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OF6NRe7rBPM"
   },
   "source": [
    "## 4. Limit Stochastic Methode\n",
    "\n",
    "In this section, you will write your own implementation of MC prediction (for estimating the action-value function)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define generate_episode (with random policy)\n",
    "Generates the starting policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lrmE4cqgrBPM"
   },
   "source": [
    "def generate_episode(env):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TllT2R7mrBPM"
   },
   "source": [
    "### Play generate_episode (with random policy)\n",
    "Play the game n episodes and print results of each episode."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vHo2DiahrBPN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cce346be-b59a-49c6-b4ff-3d964b412d76"
   },
   "source": [
    "print(\"Play 3 times and print episodes with: (state, action, reward)\")\n",
    "\n",
    "for i in range(3):\n",
    "    episode = generate_episode(env)\n",
    "    print(episode)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play 3 times and print episodes with: (state, action, reward)\n",
      "[(0, 3, 0.0), (0, 0, 0.0), (0, 3, 0.0), (0, 0, 0.0), (0, 0, 0.0), (0, 1, 0.0), (4, 1, 0.0), (8, 2, 0.0), (9, 2, 0.0), (10, 3, 0.0), (6, 2, 0.0)]\n",
      "[(0, 2, 0.0), (1, 3, 0.0), (1, 1, 0.0)]\n",
      "[(0, 1, 0.0), (4, 1, 0.0), (8, 2, 0.0), (9, 2, 0.0), (10, 3, 0.0), (6, 0, 0.0)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Monte Carlo Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define mc_prediction\n",
    "\n",
    "**Input Arguments:**\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `generate_episode`: This is a function that returns an episode of interaction.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1 inclusive (default value: `0.8`).\n",
    "- `print_n_episode`: Each n episode will be printed to system out (default value: `1`).\n",
    "\n",
    "**Return:**\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays)\n",
    "- where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "50YKZMxUrBPN"
   },
   "source": [
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=0.8, print_n_episode=1):\n",
    "\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "        # monitor progress\n",
    "        is_print = i_episode == 1 or i_episode % print_n_episode == 0\n",
    "        if is_print:\n",
    "            print(\"# ================================================================\")\n",
    "            print(\"# Episode {}/{}\".format(i_episode, num_episodes))\n",
    "            print(\"# ================================================================\")\n",
    "\n",
    "        # generate an episode\n",
    "        episode = generate_episode(env)\n",
    "\n",
    "        # obtain the states, actions, and rewards\n",
    "        states, actions, rewards = zip(*episode)\n",
    "\n",
    "        # prepare for discounting\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards))])\n",
    "        if is_print:\n",
    "            print(\"Prepare discounting:\")\n",
    "            print(\"discounts:\", discounts)\n",
    "            print(\"rewards:\", rewards)\n",
    "            print(\"states:\", states)\n",
    "            print(\"\")\n",
    "\n",
    "        # update the sum of the returns, number of visits, and action-value\n",
    "        # function estimates for each state-action pair in the episode\n",
    "        for i, (state, action) in enumerate(zip(states, actions)):\n",
    "            n_steps_after_state = len(rewards[i:])\n",
    "            if is_print:\n",
    "                print(\"rewards[i:]:\", rewards[i:])\n",
    "                print(\"discounts[:n_steps_after_state]:\", discounts[:n_steps_after_state])\n",
    "            returns_sum[state][action] += sum(rewards[i:]*discounts[:n_steps_after_state])\n",
    "            N[state][action] += 1.0\n",
    "            Q[state][action] = returns_sum[state][action] / N[state][action]\n",
    "            if is_print:\n",
    "                print(\"Q-Value:\", Q[state][action])\n",
    "                print(\"State:\", state)\n",
    "                print(\"Action:\", action)\n",
    "                print(\"\")\n",
    "    return Q"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHPlsnLSrBPO"
   },
   "source": [
    "### Test mc_prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MwjjjPh9rBPO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "640ad9e9-f621-4a27-ed21-15f4a39fd6d4"
   },
   "source": [
    "num_episodes = 3\n",
    "Q = mc_prediction_q(env, num_episodes, generate_episode)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ================================================================\n",
      "# Episode 1/3\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 1, 0, 1, 2, 2, 3)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 3\n",
      "Action: 1\n",
      "\n",
      "# ================================================================\n",
      "# Episode 2/3\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 0, 1, 1, 1, 2, 2, 1, 0, 1)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.        0.8       0.64      0.512     0.4096    0.32768   0.262144\n",
      " 0.2097152]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 1\n",
      "\n",
      "# ================================================================\n",
      "# Episode 3/3\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 0, 0, 0, 0, 0, 4, 0, 4)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.        0.8       0.64      0.512     0.4096    0.32768   0.262144\n",
      " 0.2097152]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 2\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiHkvfo0rBPO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "536dce36-179c-4b65-b2c6-b1366943ded7",
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Play mc_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ================================================================\n",
      "# Episode 1/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935\n",
      " 0.06871948 0.05497558]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 0, 1, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935\n",
      " 0.06871948 0.05497558]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935\n",
      " 0.06871948]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.        0.8       0.64      0.512     0.4096    0.32768   0.262144\n",
      " 0.2097152]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "Q-Value: 0.0\n",
      "State: 3\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "Q-Value: 0.0\n",
      "State: 3\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0\n",
      "State: 3\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0\n",
      "State: 3\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 3\n",
      "Action: 1\n",
      "\n",
      "# ================================================================\n",
      "# Episode 20000/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935\n",
      " 0.06871948 0.05497558 0.04398047 0.03518437 0.0281475  0.022518  ]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 4, 8, 9, 10, 9, 8, 9, 10, 9, 10, 9, 8, 8, 9, 13, 13, 13)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935\n",
      " 0.06871948 0.05497558 0.04398047 0.03518437 0.0281475  0.022518  ]\n",
      "Q-Value: 0.002541911500655466\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935\n",
      " 0.06871948 0.05497558 0.04398047 0.03518437 0.0281475 ]\n",
      "Q-Value: 0.008993692656612274\n",
      "State: 4\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935\n",
      " 0.06871948 0.05497558 0.04398047 0.03518437]\n",
      "Q-Value: 0.030883036303501577\n",
      "State: 8\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935\n",
      " 0.06871948 0.05497558 0.04398047]\n",
      "Q-Value: 0.06797243643311356\n",
      "State: 9\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935\n",
      " 0.06871948 0.05497558]\n",
      "Q-Value: 0.03136443510875765\n",
      "State: 10\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935\n",
      " 0.06871948]\n",
      "Q-Value: 0.005228245025824665\n",
      "State: 9\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935]\n",
      "Q-Value: 0.03087000548649588\n",
      "State: 8\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418]\n",
      "Q-Value: 0.06788954321795122\n",
      "State: 9\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773]\n",
      "Q-Value: 0.031300686256910584\n",
      "State: 10\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216]\n",
      "Q-Value: 0.06780685193510354\n",
      "State: 9\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.        0.8       0.64      0.512     0.4096    0.32768   0.262144\n",
      " 0.2097152]\n",
      "Q-Value: 0.031237196021095346\n",
      "State: 10\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "Q-Value: 0.005221533543121424\n",
      "State: 9\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "Q-Value: 0.007458189312743286\n",
      "State: 8\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.030856985661322327\n",
      "State: 8\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.09552114832889948\n",
      "State: 9\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.11088719331622897\n",
      "State: 13\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.11054282315065062\n",
      "State: 13\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 13\n",
      "Action: 0\n",
      "\n",
      "# ================================================================\n",
      "# Episode 40000/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.  0.8]\n",
      "rewards: (0.0, 0.0)\n",
      "states: (0, 1)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0013554858278058564\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 1\n",
      "\n",
      "# ================================================================\n",
      "# Episode 60000/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.   0.8  0.64]\n",
      "rewards: (0.0, 0.0, 0.0)\n",
      "states: (0, 1, 1)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0013751224174484574\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0014330102647663878\n",
      "State: 1\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 1\n",
      "\n",
      "# ================================================================\n",
      "# Episode 80000/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.        0.8       0.64      0.512     0.4096    0.32768   0.262144\n",
      " 0.2097152]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 0, 1, 1, 2, 2, 6, 10)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.        0.8       0.64      0.512     0.4096    0.32768   0.262144\n",
      " 0.2097152]\n",
      "Q-Value: 0.0013152409025092876\n",
      "State: 0\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "Q-Value: 0.0014124301218514912\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "Q-Value: 0.0013472720554214878\n",
      "State: 1\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.004442940673169506\n",
      "State: 1\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0046287253939596325\n",
      "State: 2\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.01519866155692282\n",
      "State: 2\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.07149968632708896\n",
      "State: 6\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 10\n",
      "Action: 2\n",
      "\n",
      "# ================================================================\n",
      "# Episode 100000/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.    0.8   0.64  0.512]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 0, 4, 4)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0013506568499562948\n",
      "State: 0\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0024816614776542695\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.002572053829763879\n",
      "State: 4\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100000\n",
    "print_n_episodes = 20000\n",
    "\n",
    "# obtain the action-value function\n",
    "Q = mc_prediction_q(env, num_episodes, generate_episode, print_n_episode=print_n_episodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Show Q-Value\n",
    "Show Q with state and probabilities per action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field positons:\n",
      "[ 0] [ 1] [ 2] [ 3]\n",
      "[ 4] [ 5] [ 6] [ 7]\n",
      "[ 8] [09] [10] [11]\n",
      "[12] [13] [14] [15]\n",
      "\n",
      "Field: Left        Down       Right      Up\n",
      "    0: [0.00122896 0.00248166 0.0013838  0.00135066]\n",
      "    1: [0.0013686  0.         0.00431315 0.00128558]\n",
      "    2: [0.00161723 0.01477777 0.00150881 0.00460725]\n",
      "    3: [0.0047191  0.         0.00178379 0.00163326]\n",
      "    4: [0.00257205 0.00875482 0.         0.00136826]\n",
      "    8: [0.00863522 0.         0.03130008 0.00279844]\n",
      "    9: [0.00691463 0.08152864 0.067979   0.        ]\n",
      "    6: [0.         0.069667   0.         0.00524931]\n",
      "   13: [0.         0.07726199 0.28829045 0.03046507]\n",
      "   10: [0.0321065  0.29689617 0.         0.01576785]\n",
      "   14: [0.07612059 0.29630067 1.         0.06635169]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "helper.print_field_positions()\n",
    "print()\n",
    "\n",
    "helper.print_Q(Q)\n",
    "print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvP2BeNZrBPP"
   },
   "source": [
    "## 6. Monte Carlo Control\n",
    "\n",
    "In this section, you will write your own implementation of constant-$\\alpha$ MC control.  \n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: This is a dictionary where `policy[s]` returns the action that the agent chooses after observing state `s`.\n",
    "\n",
    "**Eval vs. Improvement**\n",
    "\n",
    "Das kann nicht strikt getrennt werden.\n",
    "Fausregel:\n",
    "- Alles was Tabelle erstellt ist Evaluierung\n",
    "- Alles was Q-Vaule anpasst ist Improvment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### generate_episode_from_Q, get_probs, update_Q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_LCwfLiDrBPP"
   },
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA): # epsilon greedy\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon / nA # wenn epsilon 1 ist zu Beginn und ich habe 5 Aktionen, dann ist meine Policy [1/5, 1/5,..., 1/5]\n",
    "    # print(\"policy_s:\", policy_s)\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    # print(\"episode :\", episode)\n",
    "    # print(\"policy_s:\", policy_s)\n",
    "    # print(\"best_a  :\", best_a)\n",
    "\n",
    "    return policy_s\n",
    "\n",
    "def update_Q(env, episode, Q, alpha, gamma, is_print): # policy improvement oder control\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "        n_steps_after_state = len(rewards[i:])\n",
    "        old_Q = Q[state][actions[i]] \n",
    "        # Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "        # - Alpha mindert den Discount, wir passen die Prediction\n",
    "        #   + oder - je nachem ob wir zuwenig oder zuviel predicted haben\n",
    "        # - Q old wurde mit der gleichen Formel berechnet.\n",
    "        # - Man könnte auch den Mittelwert nehmen, aber wir wollen das \"geschickter\" machen, fein granularer!\n",
    "        Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:n_steps_after_state]) - old_Q)\n",
    "        if is_print:\n",
    "            print('state:', state)\n",
    "            print('action:', actions[i])\n",
    "            print('q-value:', Q[state][actions[i]])\n",
    "    return Q"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mc_prediction_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uoofGGbnrBPP"
   },
   "source": [
    "def mc_prediction_control(env, num_episodes, alpha, gamma=0.05,\n",
    "                          eps_start=1.0, eps_decay=.99999, eps_min=0.05,\n",
    "                          print_n_episode=print_n_episodes):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    epsilon = eps_start\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        is_print = i_episode == 1 or i_episode % print_n_episode == 0\n",
    "        if is_print:\n",
    "            print(\"# ================================================================\")\n",
    "            print(\"# Episode {}/{}\".format(i_episode, num_episodes))\n",
    "            print(\"# ================================================================\")\n",
    "        # set the value of epsilon\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "\n",
    "        # Start\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma, is_print) # prediction und eigentllich control, weil Q table die Vorlage für die nächste Action Selection ist\n",
    "\n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items()) # control - Q table wird gelesen\n",
    "    return policy, Q"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HhEQWyCrBPP"
   },
   "source": [
    "### obtain the estimated optimal policy and action-value function\n",
    "\n",
    "Use the cell below to obtain the estimated optimal policy and action-value function.\n",
    "Note that you should fill in your own values for the `num_episodes` and `alpha` parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ypyPrMAerBPQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3e728517-53fc-4620-ca10-78cae73523e7"
   },
   "source": [
    "num_episodes = 10000\n",
    "print_n_episodes = 5000\n",
    "\n",
    "alpha = 0.002\n",
    "policy, Q = mc_prediction_control(env, num_episodes, alpha, print_n_episode=print_n_episodes)\n"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ================================================================\n",
      "# Episode 1/10000\n",
      "# ================================================================\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 0.0\n",
      "state: 0\n",
      "action: 0\n",
      "q-value: 0.0\n",
      "state: 0\n",
      "action: 0\n",
      "q-value: 0.0\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 0.0\n",
      "state: 0\n",
      "action: 0\n",
      "q-value: 0.0\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 0.0\n",
      "state: 0\n",
      "action: 2\n",
      "q-value: 0.0\n",
      "state: 1\n",
      "action: 2\n",
      "q-value: 0.0\n",
      "state: 2\n",
      "action: 2\n",
      "q-value: 0.0\n",
      "state: 3\n",
      "action: 3\n",
      "q-value: 0.0\n",
      "state: 3\n",
      "action: 0\n",
      "q-value: 0.0\n",
      "state: 2\n",
      "action: 2\n",
      "q-value: 0.0\n",
      "state: 3\n",
      "action: 2\n",
      "q-value: 0.0\n",
      "state: 3\n",
      "action: 1\n",
      "q-value: 0.0\n",
      "# ================================================================\n",
      "# Episode 5000/10000\n",
      "# ================================================================\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 2.031318954299623e-11\n",
      "state: 0\n",
      "action: 2\n",
      "q-value: 8.490540837654538e-10\n",
      "state: 1\n",
      "action: 0\n",
      "q-value: 2.483103007509795e-11\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 2.0272563163910237e-11\n",
      "state: 0\n",
      "action: 0\n",
      "q-value: 3.813515800282185e-11\n",
      "state: 0\n",
      "action: 0\n",
      "q-value: 3.8058887686816205e-11\n",
      "state: 0\n",
      "action: 0\n",
      "q-value: 3.798276991144257e-11\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 2.0232018037582417e-11\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 2.019155400150725e-11\n",
      "state: 0\n",
      "action: 2\n",
      "q-value: 8.473559755979229e-10\n",
      "state: 1\n",
      "action: 0\n",
      "q-value: 2.4781368014947753e-11\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 2.0151170893504238e-11\n",
      "state: 0\n",
      "action: 1\n",
      "q-value: 1.1767795701863953e-09\n",
      "state: 4\n",
      "action: 3\n",
      "q-value: 2.5995042450517815e-11\n",
      "state: 0\n",
      "action: 2\n",
      "q-value: 8.45661263646727e-10\n",
      "state: 1\n",
      "action: 1\n",
      "q-value: 0.0\n",
      "# ================================================================\n",
      "# Episode 10000/10000\n",
      "# ================================================================\n",
      "state: 0\n",
      "action: 1\n",
      "q-value: 1.5339300679815252e-09\n",
      "state: 4\n",
      "action: 1\n",
      "q-value: 1.2016138878872155e-07\n",
      "state: 8\n",
      "action: 0\n",
      "q-value: 9.997298652340792e-08\n",
      "state: 8\n",
      "action: 2\n",
      "q-value: 6.850024234798731e-06\n",
      "state: 9\n",
      "action: 0\n",
      "q-value: 4.2977281421767445e-08\n",
      "state: 8\n",
      "action: 1\n",
      "q-value: 0.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### show Q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCvKLlxoh8fx",
    "outputId": "c310debd-5910-404b-edcc-f795b15348f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "helper.print_field_positions()\n",
    "print()\n",
    "\n",
    "helper.print_Q(Q)\n",
    "print()"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field positons:\n",
      "[ 0] [ 1] [ 2] [ 3]\n",
      "[ 4] [ 5] [ 6] [ 7]\n",
      "[ 8] [09] [10] [11]\n",
      "[12] [13] [14] [15]\n",
      "\n",
      "Field: Left        Down       Right      Up\n",
      "    0: [3.93738363e-11 1.53393007e-09 8.25093142e-10 2.61134399e-11]\n",
      "    1: [2.15769227e-11 0.00000000e+00 7.37887751e-08 2.31182956e-09]\n",
      "    2: [2.22493288e-10 3.15122315e-06 9.40876212e-11 3.55717052e-08]\n",
      "    3: [2.03049219e-08 0.00000000e+00 8.75413371e-10 3.57506431e-11]\n",
      "    4: [1.78331390e-09 1.20161389e-07 0.00000000e+00 4.55599007e-11]\n",
      "    8: [9.99729865e-08 0.00000000e+00 6.85002423e-06 2.86819002e-09]\n",
      "    9: [4.29772814e-08 1.41128326e-04 1.98636195e-04 0.00000000e+00]\n",
      "    6: [0.00000000e+00 1.52455528e-04 0.00000000e+00 3.17749363e-08]\n",
      "   10: [3.01209764e-06 9.81198583e-03 0.00000000e+00 2.31093437e-06]\n",
      "   14: [7.39389255e-05 5.11627953e-03 4.38181688e-01 1.07904445e-04]\n",
      "   13: [0.00000000e+00 1.10383530e-04 6.77356519e-03 2.68596809e-06]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### show policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:\n",
      "[0] Left\n",
      "[1] Down\n",
      "[2] Right\n",
      "[3] Up\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "{0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 8: 2, 9: 2, 6: 1, 10: 1, 14: 2, 13: 2}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.print_actions()\n",
    "print()\n",
    "\n",
    "policy\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Monte Carlo Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test random policy: Mean reward = 0.0\n",
      "Test mc policy    : Mean reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "mc_policy = lambda s: policy[s]\n",
    "\n",
    "# Tests\n",
    "print(\"Test random policy: Mean reward =\", test_performance(random_policy))\n",
    "print(\"Test mc policy    : Mean reward =\", test_performance(mc_policy))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "__The end.__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}