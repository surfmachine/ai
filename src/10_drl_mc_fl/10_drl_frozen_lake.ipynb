{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "pycharm-ec18b53a",
   "language": "python",
   "display_name": "PyCharm (python-work)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "colab": {
   "name": "Monte_Carlo_Solution.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlgE6t7crBPH"
   },
   "source": [
    "# Aufgabe 10 - Monte Carlo Methods mit Frozen Lake\n",
    "15.01.2022, Thomas Iten\n",
    "\n",
    "\n",
    "**Content**\n",
    "0. Setup\n",
    "1. Create FrozenLakeHelper\n",
    "2. Explore Blackjack Environment\n",
    "3. Play random policy and testing policy\n",
    "4. Limit Stochastic Methode\n",
    "5. Monte Carlo Prediction\n",
    "6. Monte Carlo Control\n",
    "7. Monte Carlo Testing\n",
    "\n",
    "**References**\n",
    "- https://www.deep-teaching.org/notebooks/reinforcement-learning/exercise-monte-carlo-frozenlake-gym\n",
    "- https://adityajain.me/blogs/monte-carlo-and-temporal-difference.html\n",
    "- https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Setup\n",
    "\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TJM5HsNPrBPJ"
   },
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Create FrozenLakeHelper class\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FrozenLakeHelper():\n",
    "    \"\"\"Some helper methods used throughout this notebook.\"\"\"\n",
    "\n",
    "    def render(self, env, display_mode=\"brackets\", print_result=True, legend=False):\n",
    "        \"\"\"\n",
    "        IntelliJ notebooks to not render the color of the current position correct.\n",
    "        Details see: https://youtrack.jetbrains.com/issue/PY-32191\n",
    "\n",
    "        Therfore we use this customized render methode with two simple display modes.\n",
    "\n",
    "        :param env: The current environment to render it's fields.\n",
    "        :param display_mode: display current position with \"brackets\" or in \"lowercase\"\n",
    "        :param print_result: print the last action and result\n",
    "        :param legend: print the legend\n",
    "        :return: lastaction as text and fields with marked current position\n",
    "        \"\"\"\n",
    "\n",
    "        # init data\n",
    "        row, col = env.s // env.ncol, env.s % env.ncol\n",
    "        desc = env.desc.tolist()\n",
    "        desc = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
    "\n",
    "        actions = [\"Left\", \"Down\", \"Right\", \"Up\"]\n",
    "        action = \"Init\" if env.lastaction is None else actions[env.lastaction]\n",
    "\n",
    "        # format display mode\n",
    "        indicator = None\n",
    "        if display_mode == \"brackets\":\n",
    "            desc[row][col] = \"[{}]\".format(desc[row][col])\n",
    "            desc = [[ (\" {} \".format(c) if len(c) == 1 else c) for c in line ] for line in desc]\n",
    "            indicator = \"[]\"\n",
    "        elif display_mode == \"lowercase\":\n",
    "            desc[row][col] = (desc[row][col]).lower()\n",
    "            indicator = \"lowercase\"\n",
    "\n",
    "        # print result\n",
    "        if print_result:\n",
    "            if legend:\n",
    "                print(\"Last action:\", action)\n",
    "            else:\n",
    "                print(action + \":\")\n",
    "            for line in desc:\n",
    "                for pos in line:\n",
    "                    print(pos, end=\"\")\n",
    "                print(\"\")\n",
    "            if legend:\n",
    "                print(\"Legend: S=Start, F=Frozen (safe), H=Hole, G=Goal, \" + indicator + \"=Current Position\")\n",
    "            print(\"\")\n",
    "\n",
    "        # return result\n",
    "        return action, desc\n",
    "\n",
    "\n",
    "    def print_field_positions(self):\n",
    "        print(\"Field positons:\")\n",
    "        print(\"[ 0] [ 1] [ 2] [ 3]\")\n",
    "        print(\"[ 4] [ 5] [ 6] [ 7]\")\n",
    "        print(\"[ 8] [09] [10] [11]\")\n",
    "        print(\"[12] [13] [14] [15]\")\n",
    "\n",
    "    def print_actions(self):\n",
    "        print(\"Actions:\")\n",
    "        print(\"[0] Left\")\n",
    "        print(\"[1] Down\")\n",
    "        print(\"[2] Right\")\n",
    "        print(\"[3] Up\")\n",
    "\n",
    "    def print_Q(self, Q):\n",
    "        print(\"Field: Left        Down       Right      Up\")\n",
    "        for field in Q:\n",
    "            print(f\"{field : >5}\", end=\"\")\n",
    "            print(\":\", Q[field])\n",
    "\n",
    "# Create helper instance\n",
    "helper = FrozenLakeHelper()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Explore Frozen Lake Environment\n",
    "\n",
    "### Frozen Lake environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bzabrv8irBPK"
   },
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "print(\"Action space:\")\n",
    "print(env.action_space)\n",
    "print(\"\")\n",
    "\n",
    "helper.print_actions()\n",
    "print(\"\")\n",
    "\n",
    "print(\"Observation space:\")\n",
    "print(env.observation_space)\n",
    "print(\"\")\n",
    "\n",
    "helper.print_field_positions()\n"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space:\n",
      "Discrete(4)\n",
      "\n",
      "Actions:\n",
      "[0] Left\n",
      "[1] Down\n",
      "[2] Right\n",
      "[3] Up\n",
      "\n",
      "Observation space:\n",
      "Discrete(16)\n",
      "\n",
      "Field positons:\n",
      "[ 0] [ 1] [ 2] [ 3]\n",
      "[ 4] [ 5] [ 6] [ 7]\n",
      "[ 8] [09] [10] [11]\n",
      "[12] [13] [14] [15]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJiNsXfZrBPK"
   },
   "source": [
    "### Reset and initial state"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wV5DyGz2rBPK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c08c53d3-ea0d-440a-e22d-8571956163e1"
   },
   "source": [
    "env.reset()    # reset the environment the set agent to start state\n",
    "helper.render(env, legend=True)\n",
    "print()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last action: Init\n",
      "[S] F  F  F \n",
      " F  H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "Legend: S=Start, F=Frozen (safe), H=Hole, G=Goal, []=Current Position\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jz4CvL7rBPL"
   },
   "source": [
    "## 3. Play random policy and testing policy\n",
    "\n",
    "### Play random policy\n",
    "\n",
    "The step returns:\n",
    "- new_state: new state after action (random_action) taken in current state\n",
    "- reward: reward obtained after taken action (random_action) in current state and entering new state (new_state)\n",
    "- done: bool flag, true if goal or hole is reached\n",
    "- info: slippery probability, baseline is 1/3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "belzA6_PrBPL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b83c90c8-7842-45a0-cdf4-01f1381849d2"
   },
   "source": [
    "env.reset()\n",
    "helper.render(env)\n",
    "\n",
    "# Play till done or maximum 6 iterations reached\n",
    "for i in range(1,7):\n",
    "    print(\"Step:\", i)\n",
    "    random_action = env.action_space.sample() #samples a random action\n",
    "    new_state, reward, done, info = env.step(random_action) #agent takes action (random_action)\n",
    "    helper.render(env)\n",
    "    if done:\n",
    "        break"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init:\n",
      "[S] F  F  F \n",
      " F  H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "\n",
      "Step: 1\n",
      "Down:\n",
      " S  F  F  F \n",
      "[F] H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "\n",
      "Step: 2\n",
      "Down:\n",
      " S  F  F  F \n",
      " F  H  F  H \n",
      "[F] F  F  H \n",
      " H  F  F  G \n",
      "\n",
      "Step: 3\n",
      "Up:\n",
      " S  F  F  F \n",
      "[F] H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "\n",
      "Step: 4\n",
      "Left:\n",
      " S  F  F  F \n",
      "[F] H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "\n",
      "Step: 5\n",
      "Up:\n",
      "[S] F  F  F \n",
      " F  H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "\n",
      "Step: 6\n",
      "Left:\n",
      "[S] F  F  F \n",
      " F  H  F  H \n",
      " F  F  F  H \n",
      " H  F  F  G \n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing policy\n",
    "\n",
    "The problem is said to be solved, a good policy found, if over a sufficient number of episodes (>100)\n",
    "a mean reward of >0.7 is reached.\n",
    "\n",
    "The code below tests this for a given policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def test_performance(policy, nb_episodes=100):\n",
    "    sum_returns = 0\n",
    "    for i in range(nb_episodes):\n",
    "        state  = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                sum_returns += reward\n",
    "    return sum_returns/nb_episodes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets set and test a random policy. Since later we will work with aϵ−policy we always will start by defining\n",
    "a policy as a dictionary (state-action pairs) or array (index-value pairs) and than wrap the dictionary\n",
    "or array into a function.\n",
    "\n",
    "> We see that the number is far lower than 0.7 and therefor the policy is not good/ optimal."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test random policy:\n",
      "Mean reward = 0.03\n"
     ]
    }
   ],
   "source": [
    "# Define random policy\n",
    "random_policy = lambda s: env.action_space.sample()\n",
    "\n",
    "# test random policy\n",
    "mean_reward = test_performance(random_policy)\n",
    "print(\"Test random policy:\")\n",
    "print(\"Mean reward =\", mean_reward)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OF6NRe7rBPM"
   },
   "source": [
    "## 4. Limit Stochastic Methode\n",
    "\n",
    "In this section, you will write your own implementation of MC prediction (for estimating the action-value function)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define generate_episode (with random policy)\n",
    "Generates the starting policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lrmE4cqgrBPM"
   },
   "source": [
    "def generate_episode(env):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TllT2R7mrBPM"
   },
   "source": [
    "### Play generate_episode (with random policy)\n",
    "Play the game n episodes and print results of each episode."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vHo2DiahrBPN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cce346be-b59a-49c6-b4ff-3d964b412d76"
   },
   "source": [
    "print(\"Play 3 times and print episodes with: (state, action, reward)\")\n",
    "\n",
    "for i in range(3):\n",
    "    episode = generate_episode(env)\n",
    "    print(episode)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play 3 times and print episodes with: (state, action, reward)\n",
      "[(0, 0, 0.0), (0, 3, 0.0), (0, 1, 0.0), (4, 1, 0.0), (8, 1, 0.0)]\n",
      "[(0, 1, 0.0), (4, 2, 0.0)]\n",
      "[(0, 1, 0.0), (4, 2, 0.0)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Monte Carlo Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define mc_prediction\n",
    "\n",
    "**Input Arguments:**\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `generate_episode`: This is a function that returns an episode of interaction.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1 inclusive (default value: `0.8`).\n",
    "- `print_n_episode`: Each n episode will be printed to system out (default value: `1`).\n",
    "\n",
    "**Return:**\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays)\n",
    "- where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "50YKZMxUrBPN"
   },
   "source": [
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=0.8, print_n_episode=1):\n",
    "\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "        # monitor progress\n",
    "        is_print = i_episode == 1 or i_episode % print_n_episode == 0\n",
    "        if is_print:\n",
    "            print(\"# ================================================================\")\n",
    "            print(\"# Episode {}/{}\".format(i_episode, num_episodes))\n",
    "            print(\"# ================================================================\")\n",
    "\n",
    "        # generate an episode\n",
    "        episode = generate_episode(env)\n",
    "\n",
    "        # obtain the states, actions, and rewards\n",
    "        states, actions, rewards = zip(*episode)\n",
    "\n",
    "        # prepare for discounting\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards))])\n",
    "        if is_print:\n",
    "            print(\"Prepare discounting:\")\n",
    "            print(\"discounts:\", discounts)\n",
    "            print(\"rewards:\", rewards)\n",
    "            print(\"states:\", states)\n",
    "            print(\"\")\n",
    "\n",
    "        # update the sum of the returns, number of visits, and action-value\n",
    "        # function estimates for each state-action pair in the episode\n",
    "        for i, (state, action) in enumerate(zip(states, actions)):\n",
    "            n_steps_after_state = len(rewards[i:])\n",
    "            if is_print:\n",
    "                print(\"rewards[i:]:\", rewards[i:])\n",
    "                print(\"discounts[:n_steps_after_state]:\", discounts[:n_steps_after_state])\n",
    "            returns_sum[state][action] += sum(rewards[i:]*discounts[:n_steps_after_state])\n",
    "            N[state][action] += 1.0\n",
    "            Q[state][action] = returns_sum[state][action] / N[state][action]\n",
    "            if is_print:\n",
    "                print(\"Q-Value:\", Q[state][action])\n",
    "                print(\"State:\", state)\n",
    "                print(\"Action:\", action)\n",
    "                print(\"\")\n",
    "    return Q"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHPlsnLSrBPO"
   },
   "source": [
    "### Test mc_prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MwjjjPh9rBPO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "640ad9e9-f621-4a27-ed21-15f4a39fd6d4"
   },
   "source": [
    "num_episodes = 3\n",
    "Q = mc_prediction_q(env, num_episodes, generate_episode)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ================================================================\n",
      "# Episode 1/3\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.     0.8    0.64   0.512  0.4096]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 1, 2, 2, 1)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0\n",
      "State: 2\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 1\n",
      "\n",
      "# ================================================================\n",
      "# Episode 2/3\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 4, 4, 4, 4, 0, 0, 0, 4, 8, 9, 8)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418 0.08589935]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773 0.10737418]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.        0.8       0.64      0.512     0.4096    0.32768   0.262144\n",
      " 0.2097152]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0\n",
      "State: 8\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0\n",
      "State: 9\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 8\n",
      "Action: 1\n",
      "\n",
      "# ================================================================\n",
      "# Episode 3/3\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 0, 0, 4, 0, 4, 4)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 2\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiHkvfo0rBPO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "536dce36-179c-4b65-b2c6-b1366943ded7",
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Play mc_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ================================================================\n",
      "# Episode 1/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.   0.8  0.64]\n",
      "rewards: (0.0, 0.0, 0.0)\n",
      "states: (0, 4, 4)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 2\n",
      "\n",
      "# ================================================================\n",
      "# Episode 20000/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 4, 4, 8, 8, 9)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "Q-Value: 0.0022036316276834024\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.0022917868652489338\n",
      "State: 4\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.007733283230949169\n",
      "State: 4\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.005860983593635802\n",
      "State: 8\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.02939697899632101\n",
      "State: 8\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 9\n",
      "Action: 3\n",
      "\n",
      "# ================================================================\n",
      "# Episode 40000/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.  0.8]\n",
      "rewards: (0.0, 0.0)\n",
      "states: (0, 4)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0022077364530641162\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 2\n",
      "\n",
      "# ================================================================\n",
      "# Episode 60000/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.     0.8    0.64   0.512  0.4096]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 0, 0, 4, 4)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.0011851616920802344\n",
      "State: 0\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0012010787155329427\n",
      "State: 0\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.002213566899453488\n",
      "State: 0\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0021922252747764858\n",
      "State: 4\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 4\n",
      "Action: 2\n",
      "\n",
      "# ================================================================\n",
      "# Episode 80000/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.   0.8  0.64]\n",
      "rewards: (0.0, 0.0, 0.0)\n",
      "states: (0, 1, 1)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.0012977481318263266\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.0013059448378780053\n",
      "State: 1\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 1\n",
      "Action: 1\n",
      "\n",
      "# ================================================================\n",
      "# Episode 100000/100000\n",
      "# ================================================================\n",
      "Prepare discounting:\n",
      "discounts: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773]\n",
      "rewards: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "states: (0, 0, 1, 1, 1, 2, 3, 2, 6, 10)\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216 0.13421773]\n",
      "Q-Value: 0.0012404820354830778\n",
      "State: 0\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.         0.8        0.64       0.512      0.4096     0.32768\n",
      " 0.262144   0.2097152  0.16777216]\n",
      "Q-Value: 0.0013074900492826417\n",
      "State: 0\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.        0.8       0.64      0.512     0.4096    0.32768   0.262144\n",
      " 0.2097152]\n",
      "Q-Value: 0.0013231881475757863\n",
      "State: 1\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.       0.8      0.64     0.512    0.4096   0.32768  0.262144]\n",
      "Q-Value: 0.0013231471121238534\n",
      "State: 1\n",
      "Action: 3\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.      0.8     0.64    0.512   0.4096  0.32768]\n",
      "Q-Value: 0.00409909275779571\n",
      "State: 1\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.     0.8    0.64   0.512  0.4096]\n",
      "Q-Value: 0.0015238159056465371\n",
      "State: 2\n",
      "Action: 2\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.    0.8   0.64  0.512]\n",
      "Q-Value: 0.0045891193550060905\n",
      "State: 3\n",
      "Action: 0\n",
      "\n",
      "rewards[i:]: (0.0, 0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.   0.8  0.64]\n",
      "Q-Value: 0.013739468716091349\n",
      "State: 2\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0, 0.0)\n",
      "discounts[:n_steps_after_state]: [1.  0.8]\n",
      "Q-Value: 0.06461078571195415\n",
      "State: 6\n",
      "Action: 1\n",
      "\n",
      "rewards[i:]: (0.0,)\n",
      "discounts[:n_steps_after_state]: [1.]\n",
      "Q-Value: 0.0\n",
      "State: 10\n",
      "Action: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100000\n",
    "print_n_episodes = 20000\n",
    "\n",
    "# obtain the action-value function\n",
    "Q = mc_prediction_q(env, num_episodes, generate_episode, print_n_episode=print_n_episodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Show Q-Value\n",
    "Show Q with state and probabilities per action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field positons:\n",
      "[ 0] [ 1] [ 2] [ 3]\n",
      "[ 4] [ 5] [ 6] [ 7]\n",
      "[ 8] [09] [10] [11]\n",
      "[12] [13] [14] [15]\n",
      "\n",
      "Field: Left        Down       Right      Up\n",
      "    0: [0.00124048 0.00228962 0.00130749 0.00124187]\n",
      "    4: [0.00216649 0.00799368 0.         0.00120174]\n",
      "    1: [0.00112978 0.         0.00409909 0.00132315]\n",
      "    2: [0.00128334 0.01373947 0.00152382 0.00409319]\n",
      "    6: [0.         0.06461079 0.         0.00405445]\n",
      "    8: [0.00767378 0.         0.02987264 0.00237814]\n",
      "    9: [0.00878522 0.07570893 0.06214578 0.        ]\n",
      "    3: [0.00458912 0.         0.00142198 0.00158599]\n",
      "   10: [0.02686307 0.28545753 0.         0.01520254]\n",
      "   13: [0.         0.08639715 0.2854164  0.02803959]\n",
      "   14: [0.08469964 0.26943067 1.         0.06592151]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "helper.print_field_positions()\n",
    "print()\n",
    "\n",
    "helper.print_Q(Q)\n",
    "print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvP2BeNZrBPP"
   },
   "source": [
    "## 6. Monte Carlo Control\n",
    "\n",
    "In this section, you will write your own implementation of constant-$\\alpha$ MC control.  \n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: This is a dictionary where `policy[s]` returns the action that the agent chooses after observing state `s`.\n",
    "\n",
    "**Eval vs. Improvement**\n",
    "\n",
    "Das kann nicht strikt getrennt werden.\n",
    "Fausregel:\n",
    "- Alles was Tabelle erstellt ist Evaluierung\n",
    "- Alles was Q-Vaule anpasst ist Improvment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### generate_episode_from_Q, get_probs, update_Q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_LCwfLiDrBPP"
   },
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA): # epsilon greedy\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon / nA # wenn epsilon 1 ist zu Beginn und ich habe 5 Aktionen, dann ist meine Policy [1/5, 1/5,..., 1/5]\n",
    "    # print(\"policy_s:\", policy_s)\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    # print(\"episode :\", episode)\n",
    "    # print(\"policy_s:\", policy_s)\n",
    "    # print(\"best_a  :\", best_a)\n",
    "\n",
    "    return policy_s\n",
    "\n",
    "def update_Q(env, episode, Q, alpha, gamma, is_print): # policy improvement oder control\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "        n_steps_after_state = len(rewards[i:])\n",
    "        old_Q = Q[state][actions[i]] \n",
    "        # Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "        # - Alpha mindert den Discount, wir passen die Prediction\n",
    "        #   + oder - je nachem ob wir zuwenig oder zuviel predicted haben\n",
    "        # - Q old wurde mit der gleichen Formel berechnet.\n",
    "        # - Man könnte auch den Mittelwert nehmen, aber wir wollen das \"geschickter\" machen, fein granularer!\n",
    "        Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:n_steps_after_state]) - old_Q)\n",
    "        if is_print:\n",
    "            print('state:', state)\n",
    "            print('action:', actions[i])\n",
    "            print('q-value:', Q[state][actions[i]])\n",
    "    return Q"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mc_prediction_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uoofGGbnrBPP"
   },
   "source": [
    "def mc_prediction_control(env, num_episodes, alpha, gamma=0.05,\n",
    "                          eps_start=1.0, eps_decay=.99999, eps_min=0.05,\n",
    "                          print_n_episode=print_n_episodes):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    epsilon = eps_start\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        is_print = i_episode == 1 or i_episode % print_n_episode == 0\n",
    "        if is_print:\n",
    "            print(\"# ================================================================\")\n",
    "            print(\"# Episode {}/{}\".format(i_episode, num_episodes))\n",
    "            print(\"# ================================================================\")\n",
    "        # set the value of epsilon\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "\n",
    "        # Start\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma, is_print) # prediction und eigentllich control, weil Q table die Vorlage für die nächste Action Selection ist\n",
    "\n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items()) # control - Q table wird gelesen\n",
    "    return policy, Q"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HhEQWyCrBPP"
   },
   "source": [
    "### obtain the estimated optimal policy and action-value function\n",
    "\n",
    "Use the cell below to obtain the estimated optimal policy and action-value function.\n",
    "Note that you should fill in your own values for the `num_episodes` and `alpha` parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ypyPrMAerBPQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3e728517-53fc-4620-ca10-78cae73523e7"
   },
   "source": [
    "num_episodes = 10000\n",
    "print_n_episodes = 5000\n",
    "\n",
    "alpha = 0.002\n",
    "policy, Q = mc_prediction_control(env, num_episodes, alpha, print_n_episode=print_n_episodes)\n"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ================================================================\n",
      "# Episode 1/10000\n",
      "# ================================================================\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 0.0\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 0.0\n",
      "state: 0\n",
      "action: 2\n",
      "q-value: 0.0\n",
      "state: 1\n",
      "action: 0\n",
      "q-value: 0.0\n",
      "state: 0\n",
      "action: 1\n",
      "q-value: 0.0\n",
      "state: 4\n",
      "action: 0\n",
      "q-value: 0.0\n",
      "state: 4\n",
      "action: 3\n",
      "q-value: 0.0\n",
      "state: 0\n",
      "action: 2\n",
      "q-value: 0.0\n",
      "state: 1\n",
      "action: 1\n",
      "q-value: 0.0\n",
      "# ================================================================\n",
      "# Episode 5000/10000\n",
      "# ================================================================\n",
      "state: 0\n",
      "action: 1\n",
      "q-value: 7.934264755288397e-10\n",
      "state: 4\n",
      "action: 1\n",
      "q-value: 4.0985344517284453e-08\n",
      "state: 8\n",
      "action: 0\n",
      "q-value: 4.042179503646678e-08\n",
      "state: 8\n",
      "action: 0\n",
      "q-value: 4.0340951446393846e-08\n",
      "state: 8\n",
      "action: 0\n",
      "q-value: 4.026026954350106e-08\n",
      "state: 8\n",
      "action: 0\n",
      "q-value: 4.0179749004414057e-08\n",
      "state: 8\n",
      "action: 0\n",
      "q-value: 4.009938950640523e-08\n",
      "state: 8\n",
      "action: 3\n",
      "q-value: 2.559183530597453e-10\n",
      "state: 4\n",
      "action: 3\n",
      "q-value: 2.234100842150152e-12\n",
      "state: 0\n",
      "action: 3\n",
      "q-value: 1.2942639036158151e-11\n",
      "state: 0\n",
      "action: 2\n",
      "q-value: 1.717934259164276e-09\n",
      "state: 1\n",
      "action: 1\n",
      "q-value: 0.0\n",
      "# ================================================================\n",
      "# Episode 10000/10000\n",
      "# ================================================================\n",
      "state: 0\n",
      "action: 0\n",
      "q-value: 2.287483161533456e-11\n",
      "state: 0\n",
      "action: 2\n",
      "q-value: 1.0441544618107265e-09\n",
      "state: 1\n",
      "action: 2\n",
      "q-value: 5.439828838755925e-08\n",
      "state: 2\n",
      "action: 1\n",
      "q-value: 3.275878111980581e-06\n",
      "state: 6\n",
      "action: 3\n",
      "q-value: 7.6456374724668e-09\n",
      "state: 2\n",
      "action: 3\n",
      "q-value: 6.159163751633976e-08\n",
      "state: 2\n",
      "action: 2\n",
      "q-value: 7.83472946244173e-10\n",
      "state: 3\n",
      "action: 2\n",
      "q-value: 3.074366481647641e-10\n",
      "state: 3\n",
      "action: 0\n",
      "q-value: 3.773322705513259e-08\n",
      "state: 2\n",
      "action: 2\n",
      "q-value: 7.819060003516846e-10\n",
      "state: 3\n",
      "action: 1\n",
      "q-value: 0.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### show Q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCvKLlxoh8fx",
    "outputId": "c310debd-5910-404b-edcc-f795b15348f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "helper.print_field_positions()\n",
    "print()\n",
    "\n",
    "helper.print_Q(Q)\n",
    "print()"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field positons:\n",
      "[ 0] [ 1] [ 2] [ 3]\n",
      "[ 4] [ 5] [ 6] [ 7]\n",
      "[ 8] [09] [10] [11]\n",
      "[12] [13] [14] [15]\n",
      "\n",
      "Field: Left        Down       Right      Up\n",
      "    0: [2.28748316e-11 3.62264892e-10 1.04415446e-09 3.10779115e-11]\n",
      "    1: [6.83643675e-11 0.00000000e+00 5.43982884e-08 2.93279935e-10]\n",
      "    4: [1.83895379e-09 8.74319002e-08 0.00000000e+00 5.28860749e-13]\n",
      "    2: [4.25678227e-10 3.27587811e-06 7.81906000e-10 6.15916375e-08]\n",
      "    8: [1.34706313e-07 0.00000000e+00 5.67618970e-06 2.18751368e-09]\n",
      "    9: [1.33978443e-08 1.50794998e-04 1.21784392e-04 0.00000000e+00]\n",
      "   13: [0.00000000e+00 5.54796965e-05 5.93983146e-03 2.63293723e-06]\n",
      "    3: [3.77332271e-08 0.00000000e+00 3.07436648e-10 5.84501266e-10]\n",
      "   10: [3.02533850e-06 8.13140683e-03 0.00000000e+00 1.86178158e-06]\n",
      "   14: [8.53869508e-05 5.28563419e-03 4.05788935e-01 9.75132480e-05]\n",
      "    6: [0.00000000e+00 1.68365351e-04 0.00000000e+00 7.64563747e-09]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### show policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:\n",
      "[0] Left\n",
      "[1] Down\n",
      "[2] Right\n",
      "[3] Up\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "{0: 2, 1: 2, 4: 1, 2: 1, 8: 2, 9: 1, 13: 2, 3: 0, 10: 1, 14: 2, 6: 1}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.print_actions()\n",
    "print()\n",
    "\n",
    "policy\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Monte Carlo Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test random policy: Mean reward = 0.0\n",
      "Test mc policy    : Mean reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "mc_policy = lambda s: policy[s]\n",
    "\n",
    "# Tests\n",
    "print(\"Test random policy: Mean reward =\", test_performance(random_policy))\n",
    "print(\"Test mc policy    : Mean reward =\", test_performance(mc_policy))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "__The end.__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}