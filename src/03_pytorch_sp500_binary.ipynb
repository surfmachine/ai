{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aufgabe 03 - Stockexchange Standard and Poor 500 with PyTorch\n",
    "14.11.2021, Thomas Iten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os, os.path, pickle, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# =====================================================================================================================\n",
    "# Load and prepare data\n",
    "# =====================================================================================================================\n",
    "\n",
    "class DataHandler():\n",
    "    \"\"\"Load 'Standard and Poor 500' companies performace data and split into train- and test-dataset\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url   = 'http://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "        self.start = datetime.datetime(2010, 1, 1)\n",
    "        self.stop  = datetime.datetime.now()\n",
    "        self.Ntest = 1000\n",
    "        self.now   = time.time()\n",
    "        # data files\n",
    "        self.path = \"data/\"\n",
    "        self.stocks_fname = self.path + \"sp500_closefull.csv\"        # Standard and Poor 500 companies\n",
    "        self.train_fname  = self.path + \"sp500_train.pickle\"         # Training data\n",
    "        self.test_fname   = self.path + \"sp500_test.pickle\"          # Testdata\n",
    "\n",
    "    def load_datasets(self) -> pd.DataFrame:\n",
    "        if os.path.isfile(self.train_fname) and os.path.isfile(self.test_fname):\n",
    "            train_data = pickle.load(open(self.train_fname, 'rb'))\n",
    "            test_data  = pickle.load(open(self.test_fname, 'rb'))\n",
    "        else:\n",
    "            train_data, test_data = self.download_data()\n",
    "\n",
    "        # show results\n",
    "        print(\"train_data shape:\", train_data.shape)\n",
    "        print(\"test_data  shape :\", test_data.shape)\n",
    "\n",
    "        print(\"\\ntrain_data head:\")\n",
    "        print(train_data.head(5))\n",
    "\n",
    "        print(\"\\ntest_data head:\")\n",
    "        print(test_data.head(5))\n",
    "\n",
    "        # return result\n",
    "        return train_data, test_data\n",
    "\n",
    "    def download_data(self) -> pd.DataFrame:\n",
    "\n",
    "        # Download 'Standard and Poor 500' companies and save to CSV (once)\n",
    "        if not os.path.isfile(self.stocks_fname):\n",
    "            resp = requests.get(self.url)\n",
    "            soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "            table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "            tickers = []\n",
    "\n",
    "            for row in table.findAll('tr')[1:]:\n",
    "                ticker = row.findAll('td')[0].text\n",
    "                tickers.append(ticker)\n",
    "\n",
    "            tickers = [s.replace('\\n', '') for s in tickers]\n",
    "            data = yf.download(tickers, start=self.start, end=self.end)\n",
    "            data['Adj Close'].to_csv(self.stocks_fname)\n",
    "\n",
    "        # Read companies and add SPY\n",
    "        df0 = pd.read_csv(self.stocks_fname, index_col=0, parse_dates=True)\n",
    "\n",
    "        df_spy = yf.download(\"SPY\", start=self.start, end=self.end)\n",
    "        df_spy = df_spy.loc[:, ['Adj Close']]\n",
    "        df_spy.columns = ['SPY']\n",
    "\n",
    "        df0 = pd.concat([df0, df_spy], axis=1)\n",
    "\n",
    "        # Prepare data\n",
    "        df0.dropna(axis=0, how='all', inplace=True)\n",
    "        print(\"Dropping columns due to nans > 50%:\", df0.loc[:, list((100 * (df0.isnull().sum() / len(df0.index)) > 50))].columns)\n",
    "        df0 = df0.drop(df0.loc[:, list((100 * (df0.isnull().sum() / len(df0.index)) > 50))].columns, 1)\n",
    "        df0 = df0.ffill().bfill()\n",
    "        print(\"Any columns still contain nans:\", df0.isnull().values.any())\n",
    "\n",
    "        df_returns = pd.DataFrame()\n",
    "        for name in df0.columns:\n",
    "            df_returns[name] = np.log(df0[name]).diff()\n",
    "\n",
    "        df_returns.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "        # Split data into train and test data\n",
    "        train_data = df_returns.iloc[:-self.Ntest]\n",
    "        test_data = df_returns.iloc[-self.Ntest:]\n",
    "\n",
    "        # save data files\n",
    "        pickle.dump(train_dataset, open(self.train_fname, \"wb\"))\n",
    "        pickle.dump(test_dataset,  open(self.test_fname, \"wb\"))\n",
    "\n",
    "        # return results\n",
    "        return train_data, test_data\n",
    "\n",
    "\n",
    "    def prepare(self, dataset, device, spy_binary=True, batch_size=64):\n",
    "        \"\"\"Prepare train- and testdata for the model training and validation\"\"\"\n",
    "\n",
    "        # Convert spy to binary value 0/1\n",
    "        if spy_binary:\n",
    "            dataset.SPY = np.where(dataset.SPY >=0, 1, 0)\n",
    "            print(\"Convert spy to binary value 0/1:\")\n",
    "            print(dataset.head(5))\n",
    "\n",
    "        # Split labels and features\n",
    "        labels = dataset.SPY.values\n",
    "        features = dataset.iloc[:, :-1].values\n",
    "        print(\"\\nSplit labels and features:\")\n",
    "        print(\"- label shape    :\", labels.shape)\n",
    "        print(\"- features shape :\", features.shape)\n",
    "\n",
    "        # Convert to tensor\n",
    "        tensor_labels   = torch.tensor(labels).float().to(device)\n",
    "        tensor_features = torch.tensor(features).float().to(device)\n",
    "\n",
    "\n",
    "        # Create tensor dataloader\n",
    "        print(\"\\nCreate tensor dataloader with batch_size={}\".format(batch_size))\n",
    "        data = data_utils.TensorDataset(tensor_features, tensor_labels)\n",
    "        dataloader = DataLoader(data, batch_size=batch_size)\n",
    "\n",
    "        # return result\n",
    "        return dataloader\n",
    "\n",
    "    def printTitle(self, title):\n",
    "        print(\"\\ntitle\\b\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INITIALIZATION -----------------------------------------------------------------------\n",
      "\n",
      "Device        : cpu\n",
      "SPY binary 0/1: True\n",
      "Batch size    : 10\n",
      "Learning rate : 0.001\n",
      "Epochs        : 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- INITIALIZATION -----------------------------------------------------------------------\\n\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spy_binary = True\n",
    "batch_size = 10\n",
    "learning_rate=1e-3\n",
    "epochs = 3\n",
    "\n",
    "data_handler = DataHandler()\n",
    "\n",
    "print(\"Device        :\", device)\n",
    "print(\"SPY binary 0/1:\", spy_binary)\n",
    "print(\"Batch size    :\", batch_size)\n",
    "print(\"Learning rate :\", learning_rate)\n",
    "print(\"Epochs        :\", epochs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOAD DATA ----------------------------------------------------------------------------\n",
      "\n",
      "train_data shape: (1985, 490)\n",
      "test_data  shape : (1000, 490)\n",
      "\n",
      "train_data head:\n",
      "                   A       AAL       AAP      AAPL  ABBV       ABC      ABMD  \\\n",
      "Date                                                                           \n",
      "2010-01-04  0.007375 -0.014569 -0.002474  0.015445   0.0  0.021253  0.001145   \n",
      "2010-01-05 -0.010922  0.107246 -0.005961  0.001727   0.0 -0.007160 -0.024321   \n",
      "2010-01-06 -0.003559 -0.042314  0.008682 -0.016034   0.0 -0.009501 -0.015358   \n",
      "2010-01-07 -0.001297  0.029043 -0.000247 -0.001850   0.0 -0.016166  0.000000   \n",
      "2010-01-08 -0.000325 -0.019269  0.003945  0.006626   0.0  0.010807 -0.020446   \n",
      "\n",
      "                 ABT       ACN      ADBE  ...      XLNX       XOM      XRAY  \\\n",
      "Date                                      ...                                 \n",
      "2010-01-04  0.008668  0.013641  0.008393  ...  0.012689  0.013980  0.004539   \n",
      "2010-01-05 -0.008112  0.006162  0.016313  ... -0.012689  0.003897 -0.011959   \n",
      "2010-01-06  0.005539  0.010574 -0.002124  ... -0.006807  0.008606  0.006566   \n",
      "2010-01-07  0.008250 -0.000935 -0.019595  ... -0.010095 -0.003147  0.013005   \n",
      "2010-01-08  0.005099 -0.003986 -0.005436  ...  0.014505 -0.004020  0.000000   \n",
      "\n",
      "            XYL       YUM       ZBH      ZBRA      ZION  ZTS       SPY  \n",
      "Date                                                                    \n",
      "2010-01-04  0.0  0.003426  0.015277  0.011224  0.038231  0.0  0.016817  \n",
      "2010-01-05  0.0 -0.003426  0.031165 -0.001745  0.034652  0.0  0.002644  \n",
      "2010-01-06  0.0 -0.007174 -0.000323 -0.007717  0.083381  0.0  0.000704  \n",
      "2010-01-07  0.0 -0.000288  0.022681 -0.025318  0.106160  0.0  0.004212  \n",
      "2010-01-08  0.0  0.000288 -0.021228 -0.003256 -0.016319  0.0  0.003322  \n",
      "\n",
      "[5 rows x 490 columns]\n",
      "\n",
      "test_data head:\n",
      "                   A       AAL       AAP      AAPL      ABBV       ABC  \\\n",
      "Date                                                                     \n",
      "2017-11-20  0.016866  0.006311  0.023908 -0.001000 -0.000107 -0.017659   \n",
      "2017-11-21 -0.000715  0.019930 -0.027724  0.018420  0.011472  0.025492   \n",
      "2017-11-22 -0.017605  0.000205  0.007729  0.010457 -0.002220  0.024122   \n",
      "2017-11-24  0.009707 -0.002057 -0.004361  0.000057  0.002643 -0.002091   \n",
      "2017-11-27 -0.000721 -0.002887  0.007147 -0.005042  0.007363 -0.001355   \n",
      "\n",
      "                ABMD       ABT       ACN      ADBE  ...      XLNX       XOM  \\\n",
      "Date                                                ...                       \n",
      "2017-11-20  0.006514 -0.004871  0.007671  0.000549  ...  0.011492  0.003856   \n",
      "2017-11-21  0.020195  0.014900  0.009777  0.009878  ...  0.007816  0.003965   \n",
      "2017-11-22 -0.003614 -0.005718 -0.006099 -0.006811  ... -0.013338  0.002840   \n",
      "2017-11-24  0.004415  0.005897  0.003529  0.008493  ...  0.008820  0.003938   \n",
      "2017-11-27 -0.006479  0.000356 -0.000881  0.002923  ... -0.005366 -0.003815   \n",
      "\n",
      "                XRAY       XYL       YUM       ZBH      ZBRA      ZION  \\\n",
      "Date                                                                     \n",
      "2017-11-20 -0.016719  0.008545  0.002414  0.005094  0.001195  0.017131   \n",
      "2017-11-21  0.008693  0.006250  0.009349  0.012227 -0.001103 -0.004468   \n",
      "2017-11-22  0.008618 -0.000891 -0.003779 -0.004324 -0.001933 -0.011583   \n",
      "2017-11-24 -0.001184  0.005036 -0.000632  0.000000  0.005237  0.000000   \n",
      "2017-11-27  0.007232 -0.000739  0.005039  0.002826 -0.003672  0.001509   \n",
      "\n",
      "                 ZTS       SPY  \n",
      "Date                            \n",
      "2017-11-20  0.002258  0.001705  \n",
      "2017-11-21  0.005762  0.006521  \n",
      "2017-11-22 -0.004072 -0.000885  \n",
      "2017-11-24  0.003091  0.002307  \n",
      "2017-11-27  0.003501 -0.000499  \n",
      "\n",
      "[5 rows x 490 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- LOAD DATA ----------------------------------------------------------------------------\\n\")\n",
    "train_dataset, test_dataset = data_handler.load_datasets()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PREPARE TRAIN DATA -------------------------------------------------------------------\n",
      "\n",
      "Convert spy to binary value 0/1:\n",
      "                   A       AAL       AAP      AAPL  ABBV       ABC      ABMD  \\\n",
      "Date                                                                           \n",
      "2010-01-04  0.007375 -0.014569 -0.002474  0.015445   0.0  0.021253  0.001145   \n",
      "2010-01-05 -0.010922  0.107246 -0.005961  0.001727   0.0 -0.007160 -0.024321   \n",
      "2010-01-06 -0.003559 -0.042314  0.008682 -0.016034   0.0 -0.009501 -0.015358   \n",
      "2010-01-07 -0.001297  0.029043 -0.000247 -0.001850   0.0 -0.016166  0.000000   \n",
      "2010-01-08 -0.000325 -0.019269  0.003945  0.006626   0.0  0.010807 -0.020446   \n",
      "\n",
      "                 ABT       ACN      ADBE  ...      XLNX       XOM      XRAY  \\\n",
      "Date                                      ...                                 \n",
      "2010-01-04  0.008668  0.013641  0.008393  ...  0.012689  0.013980  0.004539   \n",
      "2010-01-05 -0.008112  0.006162  0.016313  ... -0.012689  0.003897 -0.011959   \n",
      "2010-01-06  0.005539  0.010574 -0.002124  ... -0.006807  0.008606  0.006566   \n",
      "2010-01-07  0.008250 -0.000935 -0.019595  ... -0.010095 -0.003147  0.013005   \n",
      "2010-01-08  0.005099 -0.003986 -0.005436  ...  0.014505 -0.004020  0.000000   \n",
      "\n",
      "            XYL       YUM       ZBH      ZBRA      ZION  ZTS  SPY  \n",
      "Date                                                               \n",
      "2010-01-04  0.0  0.003426  0.015277  0.011224  0.038231  0.0    1  \n",
      "2010-01-05  0.0 -0.003426  0.031165 -0.001745  0.034652  0.0    1  \n",
      "2010-01-06  0.0 -0.007174 -0.000323 -0.007717  0.083381  0.0    1  \n",
      "2010-01-07  0.0 -0.000288  0.022681 -0.025318  0.106160  0.0    1  \n",
      "2010-01-08  0.0  0.000288 -0.021228 -0.003256 -0.016319  0.0    1  \n",
      "\n",
      "[5 rows x 490 columns]\n",
      "\n",
      "Split labels and features:\n",
      "- label shape    : (1985,)\n",
      "- features shape : (1985, 489)\n",
      "\n",
      "Create tensor dataloader with batch_size=10\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- PREPARE TRAIN DATA -------------------------------------------------------------------\\n\")\n",
    "train_dataloader = data_handler.prepare(train_dataset, device, spy_binary=spy_binary, batch_size=batch_size)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PREPARE TEST DATA --------------------------------------------------------------------\n",
      "\n",
      "Convert spy to binary value 0/1:\n",
      "                   A       AAL       AAP      AAPL      ABBV       ABC  \\\n",
      "Date                                                                     \n",
      "2017-11-20  0.016866  0.006311  0.023908 -0.001000 -0.000107 -0.017659   \n",
      "2017-11-21 -0.000715  0.019930 -0.027724  0.018420  0.011472  0.025492   \n",
      "2017-11-22 -0.017605  0.000205  0.007729  0.010457 -0.002220  0.024122   \n",
      "2017-11-24  0.009707 -0.002057 -0.004361  0.000057  0.002643 -0.002091   \n",
      "2017-11-27 -0.000721 -0.002887  0.007147 -0.005042  0.007363 -0.001355   \n",
      "\n",
      "                ABMD       ABT       ACN      ADBE  ...      XLNX       XOM  \\\n",
      "Date                                                ...                       \n",
      "2017-11-20  0.006514 -0.004871  0.007671  0.000549  ...  0.011492  0.003856   \n",
      "2017-11-21  0.020195  0.014900  0.009777  0.009878  ...  0.007816  0.003965   \n",
      "2017-11-22 -0.003614 -0.005718 -0.006099 -0.006811  ... -0.013338  0.002840   \n",
      "2017-11-24  0.004415  0.005897  0.003529  0.008493  ...  0.008820  0.003938   \n",
      "2017-11-27 -0.006479  0.000356 -0.000881  0.002923  ... -0.005366 -0.003815   \n",
      "\n",
      "                XRAY       XYL       YUM       ZBH      ZBRA      ZION  \\\n",
      "Date                                                                     \n",
      "2017-11-20 -0.016719  0.008545  0.002414  0.005094  0.001195  0.017131   \n",
      "2017-11-21  0.008693  0.006250  0.009349  0.012227 -0.001103 -0.004468   \n",
      "2017-11-22  0.008618 -0.000891 -0.003779 -0.004324 -0.001933 -0.011583   \n",
      "2017-11-24 -0.001184  0.005036 -0.000632  0.000000  0.005237  0.000000   \n",
      "2017-11-27  0.007232 -0.000739  0.005039  0.002826 -0.003672  0.001509   \n",
      "\n",
      "                 ZTS  SPY  \n",
      "Date                       \n",
      "2017-11-20  0.002258    1  \n",
      "2017-11-21  0.005762    1  \n",
      "2017-11-22 -0.004072    0  \n",
      "2017-11-24  0.003091    1  \n",
      "2017-11-27  0.003501    0  \n",
      "\n",
      "[5 rows x 490 columns]\n",
      "\n",
      "Split labels and features:\n",
      "- label shape    : (1000,)\n",
      "- features shape : (1000, 489)\n",
      "\n",
      "Create tensor dataloader with batch_size=10\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- PREPARE TEST DATA --------------------------------------------------------------------\\n\")\n",
    "test_dataloader = data_handler.prepare(test_dataset, device, spy_binary=spy_binary, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# =====================================================================================================================\n",
    "# Define model (describe forward path)\n",
    "# =====================================================================================================================\n",
    "\n",
    "class NeuralNetwork(nn.Module):                 # 1. Klasse erstellen von nn.Module\n",
    "\n",
    "    def __init__(self):                         # 2. Konstruktor\n",
    "        super(NeuralNetwork, self).__init__()   # Super Konstruktor aufrufen\n",
    "        self.linear_relu_stack = nn.Sequential( # Layer 2..n, einfache Netzwerk übereinander (Sequential)\n",
    "            nn.Linear(489, 512),                # Input 28x28, Output = 512\n",
    "            nn.Dropout(.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),                 # Input 512 (dito Output von oben)\n",
    "            nn.Dropout(.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),                  # Input 512 (dito Output von oben)\n",
    "            nn.Dropout(.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)                    # Prediction für 1 Kategorie\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# =====================================================================================================================\n",
    "# Train and test model\n",
    "# =====================================================================================================================\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.unsqueeze(1).to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.unsqueeze(1).to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            pred = (pred>0.5).float()\n",
    "            correct += (pred == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Accuracy: {(100*correct):>0.1f}%, Avg test loss: {test_loss:>8f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEFINE MODEL, LOSS AND OPTIMZER ------------------------------------------------------\n",
      "\n",
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=489, out_features=512, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=512, out_features=64, bias=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- DEFINE MODEL, LOSS AND OPTIMZER ------------------------------------------------------\\n\")\n",
    "\n",
    "model = NeuralNetwork().to(device)      # Angabe wo das ausgeführt werden soll\n",
    "print(model)\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # step, update von  params = params - lr * grad\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRAIN AND TEST MODEL -----------------------------------------------------------------\n",
      "\n",
      "Epoch 1\n",
      "------------------------------\n",
      "loss: 0.650895  [    0/ 1985]\n",
      "loss: 0.683994  [  100/ 1985]\n",
      "loss: 0.631473  [  200/ 1985]\n",
      "loss: 0.616704  [  300/ 1985]\n",
      "loss: 0.384311  [  400/ 1985]\n",
      "loss: 0.411925  [  500/ 1985]\n",
      "loss: 0.320295  [  600/ 1985]\n",
      "loss: 0.323409  [  700/ 1985]\n",
      "loss: 0.100646  [  800/ 1985]\n",
      "loss: 0.090129  [  900/ 1985]\n",
      "loss: 0.635119  [ 1000/ 1985]\n",
      "loss: 0.076691  [ 1100/ 1985]\n",
      "loss: 0.358501  [ 1200/ 1985]\n",
      "loss: 0.309440  [ 1300/ 1985]\n",
      "loss: 0.105265  [ 1400/ 1985]\n",
      "loss: 0.163056  [ 1500/ 1985]\n",
      "loss: 0.368975  [ 1600/ 1985]\n",
      "loss: 0.112179  [ 1700/ 1985]\n",
      "loss: 0.247933  [ 1800/ 1985]\n",
      "loss: 0.897221  [ 1900/ 1985]\n",
      "Test Accuracy: 89.4%, Avg test loss: 0.274684\n",
      "Done!\n",
      "Epoch 2\n",
      "------------------------------\n",
      "loss: 0.041994  [    0/ 1985]\n",
      "loss: 0.011813  [  100/ 1985]\n",
      "loss: 0.151810  [  200/ 1985]\n",
      "loss: 0.031329  [  300/ 1985]\n",
      "loss: 0.222944  [  400/ 1985]\n",
      "loss: 0.249220  [  500/ 1985]\n",
      "loss: 0.049791  [  600/ 1985]\n",
      "loss: 0.162805  [  700/ 1985]\n",
      "loss: 0.100447  [  800/ 1985]\n",
      "loss: 0.173083  [  900/ 1985]\n",
      "loss: 0.431823  [ 1000/ 1985]\n",
      "loss: 0.080472  [ 1100/ 1985]\n",
      "loss: 0.314196  [ 1200/ 1985]\n",
      "loss: 0.063627  [ 1300/ 1985]\n",
      "loss: 0.067279  [ 1400/ 1985]\n",
      "loss: 0.048220  [ 1500/ 1985]\n",
      "loss: 0.079944  [ 1600/ 1985]\n",
      "loss: 0.084875  [ 1700/ 1985]\n",
      "loss: 0.084068  [ 1800/ 1985]\n",
      "loss: 0.626100  [ 1900/ 1985]\n",
      "Test Accuracy: 90.0%, Avg test loss: 0.285071\n",
      "Done!\n",
      "Epoch 3\n",
      "------------------------------\n",
      "loss: 0.046506  [    0/ 1985]\n",
      "loss: 0.010996  [  100/ 1985]\n",
      "loss: 0.222161  [  200/ 1985]\n",
      "loss: 0.000461  [  300/ 1985]\n",
      "loss: 0.038685  [  400/ 1985]\n",
      "loss: 0.209963  [  500/ 1985]\n",
      "loss: 0.117597  [  600/ 1985]\n",
      "loss: 0.071082  [  700/ 1985]\n",
      "loss: 0.019766  [  800/ 1985]\n",
      "loss: 0.094324  [  900/ 1985]\n",
      "loss: 0.595625  [ 1000/ 1985]\n",
      "loss: 0.059966  [ 1100/ 1985]\n",
      "loss: 0.092994  [ 1200/ 1985]\n",
      "loss: 0.058178  [ 1300/ 1985]\n",
      "loss: 0.019844  [ 1400/ 1985]\n",
      "loss: 0.040800  [ 1500/ 1985]\n",
      "loss: 0.025434  [ 1600/ 1985]\n",
      "loss: 0.080374  [ 1700/ 1985]\n",
      "loss: 0.050555  [ 1800/ 1985]\n",
      "loss: 0.283360  [ 1900/ 1985]\n",
      "Test Accuracy: 90.1%, Avg test loss: 0.325604\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- TRAIN AND TEST MODEL -----------------------------------------------------------------\\n\")\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch {e+1}\\n------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "    print(\"Done!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}